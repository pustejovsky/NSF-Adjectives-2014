\documentclass[10pt]{article}

\usepackage{fullpage}
\usepackage{palatino}
\usepackage{amssymb}
\usepackage{graphicx,epsfig,color}
\usepackage{epstopdf}
%\usepackage{lingstyle}
\usepackage{array}
\usepackage{tabularx}
\usepackage{url}
\usepackage{lingmacros}
\usepackage{multicol}
\usepackage{wrapfig}

\setlength{\parskip}{1 mm}

\renewcommand{\baselinestretch}{1}
\newcommand{\concept}[1]{\textit{#1}}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand{\labelenumii}{\roman{enumii}.}
\newcommand{\EF}{\mbox{\sf F}}
\newcommand{\WSD}{\textsc{WN$^+_{\sc F}$}}
\newcommand{\ANNO}{\textsc{EvokeDB}}
\newcommand{\MID}{\mbox{$\ |\ $}}
\newcommand{\PROB}{\mbox{\sf Pr\,}}
\newcommand{\SIM}{\mbox{\textsf{sim}}}
\newcommand{\SSIM}{\mbox{\footnotesize \textsf{sim}}}
\newcommand{\PROBS}{\mbox{{\footnotesize \textsf{Pr}}}}

\newcommand{\REF}{\textsf{BibRef}}
\newcommand{\WNP}{\textsc{WN$^+$}}
\newcommand{\WN}{\textsc{WordNet}}
\newcommand{\ARC}{arc}\newcommand{\ARCC}{Arc}
\newcommand{\NLP}{NLP}
\newcommand{\MPS}{4.85in}
\newcommand{\APP}{application}\newcommand{\APPC}{Application}
\newcommand{\ANN}{annotater}
\newcommand{\TYP}{exemplification}\newcommand{\TYPC}{Exemplification}
\newcommand{\CORE}{core}
\newcommand{\DELEGATE}{delegate}
\newcommand{\CONTEXT}{context}
\newcommand{\SIMDOM}{\mbox{$\cal D$}}

\let\origdescription\description
\renewenvironment{description}{
 \setlength{\leftmargini}{1em}
 \origdescription
 \setlength{\itemindent}{-1em}
 \setlength{\labelsep}{\textwidth}
}{\endlist}

\pagenumbering{arabic}
\renewcommand{\thepage} {D--\arabic{page}}

\newcommand{\moveup}{\vspace*{-1.8mm}}
\newcommand{\weemoveup}{\vspace*{-0.8mm}}
\newcommand{\miniskip}{\vspace*{1mm}}

\begin{document}

\begin{center}
{\LARGE{\bf Project Description}}\\*[3mm]
\vspace {-6mm}
 
\subsection*{IIS-RI MEDIUM: Collaborative Research\\
%Inference Discovery through Paired Annotation: A Case Study with Adjectives}
Discovering Inference Patterns for Adjectives}
% in Language}
%Lexical Inference Patterns for Adjectives in Natural Language}
\end{center}

\vspace {-2mm}

\section{\label{intro}Introduction}

\vspace {-3mm}

%%Effective 
%%human 
%%communication relies on the ability of speakers to recover information that is not explicitly expressed in an utterance. %According to \cite{graesser1985implicit}'s estimate, 90\% of what 
%%is communicated is implicit and must be inferred. \cite{grice1975logic}, in his investigation 
%%of the pragmatics of human communication, postulated principles of cooperative 
%%conversation, by which context-dependent meanings can be recognized and interpreted 
%%without being expressed literally or even at all. 
%%While some of this is due to the pragmatics of the situation and the conversation, much of such covert information can be identified as ``semantic inferences'', and as such, can be associated with identifiable structural or lexical patterns in natural language.
%%An understanding of how speakers identify and exploit systematic covert inferences in language can enrich our models of compositionally derived inferences. At the same time, it can enhance the capabilities of natural language understanding systems to read beyond the surface forms of the text.
%
%Linguistic communication relies on the ability of language users to identify the often implicit semantic dependencies in an utterance. 
%%While some semantic relations follow directly from the syntax accompanying a verb and its arguments, there are many dependencies that are not explicitly expressed by a simple interpretation of the structure. 
%%
%%Effective communication relies on the ability of language users to recover information that is not explicitly expressed in utterances. 
%%
%%Much of this covert information can be identified as 
%It is on the basis of decoding of the structural and lexical patterns encoding these implicit dependencies that we make ``semantic inferences''.
%An understanding of how speakers identify and exploit such structural and lexical inference patterns thus has the potential to enrich models in theoretical linguistics. 
%At the same time, it can support a major goal of current {\sc nlp} research: allowing natural language understanding systems to recover more of the rich semantic information encoded in the text: did the event mentioned in the text actually happen? was it considered desirable?
%%Are they deemed desirable? 
%what qualities are being attributed to entities in the text?
%%To answer  questions such as these, careful analysis of the lexical semantics of adjectives is needed.
%%% still seems to be selling the whole enterprise short. There are MANY more questions enabled by adjective annotation and inferences.
%

%Identifying such inferences in a computational setting requires the reconstruction of %subtle 
%lexical, syntactic, 
%%constructional,
%and contextual information in texts, for the same reason that effective communication requires language users to recover information not expressed explicitly. Existing systems typically rely on training or development corpora annotated manually or automatically, using information derived from lexical classes or from syntax-semantics correspondences (SYSTEMS REFS). 
%%found in the linguistic literature. 
%To date, most research in the field has concentrated on identifying events and their participants; this has led to a focus on
%identifying verbs and verb senses, and nouns within {\it named entities}. While many of these semantic relations follow directly from the syntax accompanying a verb and its associated semantic roles, there are many other ones that cannot be read off the syntactic structure as readily. These tend to express the stance of the author towards the events described rather than encode events. One class that has been neglected as a consequence of the focus on 'events', are adjectives. 
%
%%What is sorely missing, however, is any serious computational examination of the semantic behavior of adjectives in language. 
%
%% has concentrated on coarse-grained semantic tasks such as identifying events and their participants; this has led to a focus on verbs and nouns. 
%
%%More recently, attention has shifted to exploiting information about the stance of the participants in the textual information exchange.  

%Supervised or unsupervised annotation is typically used to improve automatic inference based on the lexical items in texts. These annotations reflect the inferential potential associated with lexical items. Some aspects of this potential have been studied in the linguistic literature; computational approaches tend to take these results for granted. 
%In the case of nouns, the WordNet hierarchies have proved useful in numerous studies (e.g., \cite{snow04}); for verbs, lists of special inference patterns have been constructed starting from the work of \cite{kiparsky+kiparsky:1970,karttunen:1971} by \cite{nairn+condoravdi+karttunen:2006,sauri:2008phd,factbank:2009,lotan:2012}. Information about the inferential properties of adjectives is, however, much less easy to come by. 
%%Our preliminary studies show that, 
%For the categories of adjectives that we are interested in, the existing resources have severe shortcomings or are non-existent, in part because 
%%One of the reasons is that 
%the contribution of adjectives tends to be more subtle and more dependent on the rest of the linguistic context. This difficulty requires
%%, in our view, 
%adopting a more careful methodology 
%%than the one that has been used up to now 
%for syntactic and semantic lexical categorization tasks. The availability of
%%, on the one hand, 
%digital corpora (the biggest one being the Web itself) and
%%, on the other, 
%crowd-sourcing techniques to elicit the judgments of a larger and more diverse group of native speakers allow us to go beyond the narrow base that traditionally lexical studies were based on. Moreover, the development of statistical modeling techniques allow us to test theoretical hypotheses with large datasets. This should allow us to obtain better data to feed into automatic inferencing systems (such as BiuTee \cite{stern+dagan:2011} or \cite{clark2007role}).


%The research proposed here addresses this problem
%%in two ways: (a) we provide 
%by providing a coherent methodology for how to identify and annotate the semantic inferences associated with adjectives in texts. This will enable language comprehension tasks in computational linguistics to examine the rich semantics of adjectives in a general and consistent manner, much like is currently done with verbs and nouns. 
%
%The fact that most of the major lexical resources in the field (besides WordNet) do not even attempt to model adjective senses reflects  that there has been no consistent methodology on how to characterize adjectival meaning and behavior, and, as a result, no serious attempt to annotate adjective semantics and word senses in text. For example, OntoNotes (Weischedel et al, 2012) examines sense distinctions for verbs, nouns, and nominalized verbs, but not adjectives.  ANOTHER EXAMPLE?
%As a consequence, most of the shared tasks in the SemEval challenges in the field have had to largely ignore the semantic role and inferential impact contributed by adjectives in the language. 
%
%%We address questions whether or not mentioned events are implied to have
%%occurred, to be likely to occur, to be desirable, and for entities,
%%how their qualities compare to those of other entities.
%%% Our work goes beyond the mere identification of entities and events and addresses questions as to whether or not events have  occurred, might be likely to occur, are desirable, and for entities, how their qualities compare to those of other entities. 
%%These questions point naturally to 
%%Our solution is to assign
%%a more prominent role to the semantics of adjectives and adverbs.
%%for how adjectives and adverbs are semantically interpreted. 

%Currently there is very little  linguistic information available for annotating adjectives, and it is not even clear how their relations should be modeled. (cf. RASKIN/NIRENBURG)
%Moreover, the few studies reported in the linguistic literature do not address the questions that motivate this proposal: 
%%they ignore 
%%important semantic interactions between the various textual elements as well as 
%\begin{itemize}
%\moveup
%\item they ignore the usage of linguistically untrained  speakers. 
%\moveup
%\item they generally treat inference as an all-or-nothing matter. 
%\moveup
%\item they consider only a very limited set of the semantic interactions of the textual elements involved
%\moveup
%\end{itemize}
%%, ignoring inferences that arise reliably but are not entailments.
%% non-negligible groups of speakers. 
%
%We propose to develop a methodology that addresses these shortcomings and to validate it on three subsets of adjectives.
%This approach will result in a more complete characterization of how  these adjectives are interpreted, 
%%in the wild, 
%through a better integration of lexical resources and corpus annotation. 
% 
%%Our proposed research then, consists  in developing 
%We develop a methodology for the discovery and exploitation of systematic linguistic inferences identified with specific lexical classes in natural language by
%constructing an inferential model for adjectival semantics
%% and using paired annotation. 
%Specifically, we propose to:
%%% We develop this methodology in the construction of an inferential model for adjectival semantics in natural language. To this end, the specific aims of this proposal are:
%
%%%% end of edits [JDP 9/23/14]
%\begin{itemize}
%
%\moveup
%\item Establish an initial model for each adjective class, combining existing background from linguistic theory with data mining over large corpora to identify structure-to-inference mappings, i.e., syntactic and distributional correlates of judgments by trained annotators.
%\moveup
%\item Create larger labeled data sets using linguistically untrained annotators recruited on crowdsourcing platforms, notably Amazon Mechanical Turk (AMT).
%%\moveup 
%%\item Draw systematic comparisons between the judgments of trained and untrained annotators, iteratively refining experimental techniques with the goal of establishing practical methods to create larger-scale annotated corpora than is achievable using small groups of trained annotators; 
%\moveup
%
%\item Revise and enrich models in light of results, with the goal of enriching lexical resources, e.g.,  WordNet.  
%
%\end{itemize}
%\moveup
%


% \vspace {-1mm}
% 
%We concentrate on three diverse semantic types of adjectives, 
%in order to: (a) test the applicability of the methodology to different semantic classes; and (b) to articulate just how the structure-to-inference mapping can be modeled within each lexical class. The adjective types studied are: 
%(i) dimensional and evaluative adjectives with scalar values and associated scalar implicatures, e.g., \textit{pretty, beautiful, large, huge}; (ii) veridicity-related
%%evidentiality 
%adjectives, showing varying implications of veridicity over a clausal complement, e.g., \textit{rude, annoying, likely}, etc.; and (iii) intensional adjectives, introducing implications of modal subordination, e.g., \textit{alleged, supposed, so-called}. 
%
%Together these classes cover attributive as well as predicative uses of adjectives and the major non-intersective classes that have rich and unexpected inferential properties (intersective adjectives do not present inferential challenges beyond these addressed in event-focused research) \footnote{Classic 
%semantic field analysis (cf. \cite{dixon:91,lyons:77,raskin1995lexical}) categorizes attributes denoted by adjectives according to a thematic organization lexically encoded in the language.\footnote{It should be noted that \cite{raskin1995lexical} , however, also discuss inferential patterns for distinct classes.}
%\noindent 
%An alternative is to adopt a more formally descriptive and operational distinction which grouping adjectives into inferential classes. \cite{amoia2006adjective,amoia2008test} make such a move, adopting a four-class distinction based on inferential properties noted by \cite{Kamp75twotheories,Kamp95prototypetheory}:
%\vspace {-3mm}
% \enumsentence{
% Suppose the construction, [A N], is used to describe object O. Then A can be classified as: \\
%\vspace{-.25in}
%\begin{multicols}{2}
%a. {\sc intersective}: 
%O is both A and N. 
%\\
%b. {\sc subsective}: 
%O is A relative to N, but not necessarily in general. 
%\\
%c. {\sc privative}: 
%O is not an N, by virtue of being A. 
%\\
%d. {\sc non-subsective}: 
%the description does not determine whether O is N.  
%\end{multicols}
%}
%
%\vspace {-2.5mm}
%}

%The work will start out with an initial structure-to-inference mapping (SIM) for each adjective PATTERN 
%%(some work already done in FactBank will be repurposed here). Unlike most previous efforts, this standard is not the end product to be used in learning but constitutes a baseline: 
%Based in these, we develop finite-state templates to heuristically consult large corpora to ascertain whether our SIMs contain all the relevant information. If not we revise them. Based on the revised SIMs
%%we will construct paired annotations by comparing the baseline structure-to-inference mappings to 
%we construct test sets to elicit inferential judgments from non-expert native speakers, in particular Mechanical Turk workers (MTurkers). Our preliminary studies lead us to expect that they will expose further factors introducing unpredicted variance. We hypothesize that an important part of this variance is caused by textual factors that are abstracted away in linguistic studies, but are important to explain the non-expert judgments. 
%
%%We will use these differential measurements in judgment (trained linguist vs.\ non-expert annotator) to classify inferences according to how stable they are regardless of linguistic context and which (if any) contextual factors contribute to blocking the inference. 
%
%On the basis of this study, we will develop a corpus of RTE type inference pairs that our experts will annotate with the expected responses and test this again with non-expert native speakers. We then build a conceptual model to explicitate our assumptions and a statistical model to gauge how well our distinctions explain the behavior of the MT annotators. This approach will allow us to account for the interactions of different structural and lexical factors instead of seeing them as independent from each other. 
%
%For each class of adjectives we will develop
%\begin{itemize}
%\moveup
%\item annotation guidelines for crowd source annotation
%\moveup
%\item an annotated corpus in the RTE style (TH paris)
%\moveup
%\item conceptual model that captures the behavior of the adjective classes studied
%\moveup
%\item statistical models that evaluate how well the hypothesized factors explain the data of the experiments
%\moveup
%\item a RTE style evaluation 
%\moveup
%\end{itemize} 
%
%%%The contributions of this research are significant to the computational linguistics community 
%This research is significant
%in two major respects.  First, it lays theoretical and methodological groundwork for a large-scale annotation of adjectives in order to support automatic systems in inferencing tasks. Second, it leads to
%% contribute to 
%a more sophisticated theory of textual inferencing. By studying inferencing ``in the wild'' 
%%and how it differs from the baseline established from gold standard corpora, 
%we begin to identify the pragmatic factors contributing to the interpretation of lexical items in richer contexts.
%
Linguistic communication relies on the ability of language users to identify the often implicit semantic dependencies in an utterance. It is on the basis of 
%decoding 
interpreting the lexical and structural  patterns encoding these implicit dependencies that we make ``semantic inferences''. An understanding of how speakers identify and exploit such lexical and structural inference patterns has the potential to significantly enrich models in theoretical linguistics. 
At the same time, it furthers a major goal of current {\sc nlp} research: allowing natural language understanding systems to recover more of the rich semantic information encoded in the text, e.g.: Did the event mentioned in the text actually occur? When did it happen? What sentiment was conveyed regarding the event? What qualities are being attributed to the participants in the event? 
%was it considered desirable? what qualities are being attributed to entities in the text?

Identifying such inferences in a computational setting requires the reconstruction of lexical, syntactic, and contextual information in texts, for the same reason that effective communication requires language users to recover information not expressed explicitly. Existing systems typically rely on training or development corpora annotated manually or automatically, using information derived from lexical classes or from syntax-semantics correspondences.
% (SYSTEMS REFS). 
To date, most research in the field has concentrated on identifying events and their participants; this has led to a focus on identifying verbs and verb senses, and nouns within {\it named entities}. While many of these semantic relations follow directly from the syntax accompanying a verb and its associated semantic roles, there are many crucial semantic relations that cannot be read off the syntactic structure as readily: for example, comparative information about graded properties of the  individuals and events described, and information about the stance of the author of a text towards the events described. This information that is more \emph{evaluative} in nature is frequently conveyed by the use of adjectives. For instance, in scalar adjectives, the entity characterized is evaluated against others of the same class; in intensional adjectives the truth of the description given to an entity is evaluated and and predicative evaluative, emotive and epistemic adjectives express both the author's stance with respect to the truth of the event described in the complement and, in the case of evaluative and emotive ones, an evaluation of the character of the protagonist of that event. 

Adjectives make up a significant portion of typical English texts, usually between 5\%, and 10\%. They influence the interpretation of texts in ways that make their study particularly interesting, now that information extraction is going beyond simple entity and event identification, in order to take into account both modal and subjective factors in the text. For instance, Wiebe \cite{wiebe2000learning} notes that the presence of an adjective is a good indicator of the subjective stance of an utterance. This statistic could be improved upon if instead of looking at a simple POS tag, we had  tagging and patterns based on a more refined  classification of adjective types. For example, the most common treatment of adjectives is to assume that they are, by default, {\sc intersective} -- that is, an  object described by [A N] is both ``an A" and ``an N". Hence, an ``American woman'' is both an American and a woman; a ``tall building" is both tall and a building, and so on. 
It is usually assumed that such uses are objective, but in fact most adjectives are rarely interpreted  intersectively. Nevertheless, most computational treatments  within the field have  ignored the non-intersective behavior of adjectives in language.   When exceptions are noted, they are usually handled by simply blocking the inference: e.g., an ``alleged thief" cannot be inferred to be a thief, or to be alleged (whatever this would mean). 

%% make reference to 
% \cite{giegerich2005associative}
% jdp


%This model limits the influence of adjectives on inferences that can be derived to the word itself or---for non-intersective adjectives---the word and the noun that it modifies. 


%However,
As we demonstrate here, however,  the inferential reach of adjectives extends far beyond the treatment of the adjective on its own,  even if we extend the classification to  account for subsective, privative, and non-subsective adjectives.\footnote{
In the construction, [A N], an adjective A is:  {\sc subsective} if the object is A relative to N, but not necessarily in general; {\sc privative} if the object 
 is not an N, by virtue of being A; and 
 {\sc non-subsective} if the A  does not determine whether object is N.}
 Each of the three classes of adjectives that we propose to study below---scalar, veridicity, and intensional---will typically influence the interpretation of significant portions of the surrounding linguistic context, generating robust inferences. They do this by taking semantic scope over significant portions of text in a context-sensitive way, or by generating entailments and implicatures involving comparisons that are not overtly present in the text. 
%[don't understand the original text: my first reaction: how can something be an adjective that is not in the text? I kinda see what you want to say but i don't think it is a clear way to state it.]
%
As we will demonstrate below, two of the examples given above---\emph{tall} and \emph{alleged}---are associated with inferences of this type, and are not well-treated on the standard model. Indeed, our preliminary investigations suggest that simply intersective, non-scalar adjectives are rare in natural corpora.

%Considering the importance of adjectives in conveying information in texts, their semantic variability, and the poor fit of a one-size-fits-all model of their meaning, it is perhaps surprising that 


In the case of nouns, the WordNet hierarchies have proved useful in numerous studies (e.g., \cite{snow04}); for verbs, there is VerbNet and FrameNet as well as lists of special inference patterns have been constructed starting from the work of \cite{kiparsky+kiparsky:1970,karttunen:1971} by \cite{nairn+condoravdi+karttunen:2006,sauri:2008phd,factbank:2009,lotan:2012}. Information about the inferential properties of adjectives is, however, much less easy to come by. For the categories of adjectives that we are interested in, the existing resources have severe shortcomings or are non-existent, in part because the contribution of adjectives tends to be more subtle and more dependent on the rest of the linguistic context. A few resources have been developed in the area of sentiment analysis (e.g. H\&K) but even in those, as in the few proposals to annotate adjectives (cf. RASKIN/NIRENBURG), there is no attempt to model adjective senses, and no consistent methodology has been developed for characterizing and encoding their inferences or their relations. While WordNet does contain a certain amount of adjective information, the `dumbbell' model is fairly impoverished for these purposes, as discussed below. As a consequence most of the shared tasks in the SemEval challenges in the field have had to largely ignore the semantic role and inferential impact contributed by adjectives in the language.

There is thus a serious gap between current practice and available resources, on the one hand, and the needs of robust inference-enabled systems on the other. The research proposed here addresses this issue with an in-depth investigation of the lexical semantics of three well-attested adjective classes, using a new and coherent methodology for identifying and annotating the semantic inferences associated with adjectives in texts. The methodology proposed goes beyond what is the practice in the NLT annotation community by integrating corpus research and experimental data and by relying on the judgments of non expert native speakers rather than on those of students in linguistics. We expect that this methodology will be of a general use to computational linguists in obtaining richer information that is currently the case but it is motivated by the fact that the use of adjectives specifically tends to be more oriented towards the evaluation of states of affairs rather than to the plain description of them and that this leads to more variation in their interpretation. 

%contained in adjectives in a general and consistent manner, as is already done with verbs and nouns. 

%Currently there are few proposals available for annotating adjectives, and in the existing ones it is not made clear how  their relations should be modeled. (cf. RASKIN/NIRENBURG)
%Moreover, the few studies reported in the linguistic literature do not address the questions that motivate this proposal: 

The availability of digital corpora (the biggest one being the Web itself) and crowd-sourcing techniques to elicit the judgments of a larger and more diverse group of native speakers allow us to go beyond the narrow base that traditionally lexical studies were based on. Moreover, the development of statistical modeling techniques allow us to test theoretical hypotheses with large datasets. Our methodology aims to discover systematic linguistic inferences of specific lexical classes in natural language through an integration of lexical resources and corpus annotation and to construct  inferential model for these classes of lexical items. 

It differs from current annotation methodology by
\begin{itemize}
\moveup
\item using many linguistically untrained speakers instead of a few experts. 
\moveup
\item treating inference probabilistically instead of as an all-or-nothing matter. 
\moveup
\item taking into consideration a rich set of semantic interactions of the textual elements involved instead of artificially constraining them.
\moveup
\end{itemize}


%Specifically, we propose to:
%%%% end of edits [JDP 9/23/14]
%\begin{itemize}
%\moveup
%\item Establish an initial model for each adjective class, combining existing background from linguistic theory with data mining over large corpora to identify structure-to-inference mappings, i.e., syntactic and distributional correlates of judgments by trained annotators.
%\moveup
%\item Create larger labeled data sets using linguistically untrained annotators recruited on crowdsourcing platforms, notably Amazon Mechanical Turk (AMT).
%\moveup
%\item Revise and enrich models in light of results, with the goal of enriching lexical resources, e.g.,  WordNet. 
%\moveup
%\moveup
%\moveup
%\item Extent the current format of annotation schemes to take probabilistic information into account.  
%\end{itemize}
%\moveup
 
In this proposal, we concentrate on three diverse semantic types of adjectives, in order to: (a) test the applicability of the methodology to different semantic classes; and (b) to articulate precise conceptual models for each lexical class. The adjective types studied are: (i) dimensional and evaluative adjectives with scalar values and associated scalar implicatures, e.g., \textit{pretty, beautiful, large, huge}; (ii) veridicity-related adjectives, showing varying implications of veridicity over a clausal complement, e.g., \textit{rude, annoying, likely}, etc.; and (iii) intensional adjectives, introducing implications of modal subordination, e.g., \textit{alleged, supposed, so-called}. 

Together these classes cover attributive as well as predicative uses of adjectives and the major non-intersective classes that have rich and unexpected inferential properties\footnote{Classic semantic field analysis (cf. \cite{dixon:91,lyons:77,raskin1995lexical}) categorizes attributes denoted by adjectives according to a thematic organization lexically encoded in the language. Only \cite{raskin1995lexical} discusses inferential patterns for distinct classes.}. Quantitatively, these classes contain hundreds of the most frequently used adjectives. A list of the 2000 most used words in the BNC (AK), contains 274 adjectives. Among these, 13 are epistemic adjectives and 94 can have either emotive or evaluative propositional complements [Dan or Christiane, can you put something in about scalar ones]. 

%The three classes illustrate \emph{evaluative} uses of adjectives, the kind of use, we think, sets the typical use of that POS apart from the typical use of nouns and verbs; the choice of a scalar adjective evaluates the entity characterized compared to others of the same class; the intensional adjectives evaluate the truth of the description given to an entity and the predicative adjectives express the author's stance wrt the truth of the event described in the complement and, in the case of evaluative and emotive adjectives, an evaluation of the character of the protagonist of that event. 
%\noindent 
%An alternative is to adopt a more formally descriptive and operational distinction which grouping adjectives into inferential classes. \cite{amoia2006adjective,amoia2008test} make such a move, adopting a four-class distinction based on inferential properties noted by \cite{Kamp75twotheories,Kamp95prototypetheory}:
 
The work will start out with developing initial inferential models for our classes of adjectives based on existing literature on a on existing annotation efforts, specifically, FactBank (ref) will be used in this stage.
% standard inference corpus created by using a standard linguistic annotation effort following explicit guidelines indicating the structure-to-inference mapping for each type of adjective (some work already done in FactBank will be repurposed here). 
 
% Unlike most previous efforts, this standard is not the end product to be used in learning but constitutes a baseline: 
We will first impressionistically test these models against large corpus data. We will then construct test sets to elicit inferential judgments from non-expert native speakers, in particular Mechanical Turk workers (MTurkers). Our preliminary studies have lead us to expect that there will be variance from the baseline. We hypothesize that an important part of this variance is caused by textual factors that are abstracted away in linguistic studies, but are important to explain the non-expert judgments. 

On the basis of this study, we will develop a gold standard and test it again with non-expert native speakers. We will then build a model to gauge how well our distinctions explain the behavior of these speakers. Our approach will allow us to account for the interactions of different structural and lexical factors instead of seeing them as independent from each other. 

For each class of adjectives we will develop
\begin{itemize}
\moveup
\item annotation guidelines for crowd source annotation
\moveup
\item annotated corpora in the RTE style (TH pairs)
\moveup
\item conceptual models that capture the behavior of the adjective classes studied.
\moveup
\item statistical models that evaluate how well the hypothesized factors explain the data of the experiments.
\moveup
\moveup
\moveup
\item a RTE style corpus and an evaluation study on that corpus
\moveup
\end{itemize} 

This research is significant in two major respects. First, it develops inference models for adjectives that are richer than the ones currently envisioned. Second, it develops a new method for textual annotation, combining crowdsourcing and corpus exploitation and producing probabilistic annotations.

%First, it lays theoretical and methodological groundwork for a large-scale annotation of adjectives in order to support automatic systems in inferencing tasks. Second, it leads to a more sophisticated theory of textual inferencing. By studying inferencing ``in the wild'' we begin to identify the pragmatic factors contributing to the interpretation of lexical items in richer contexts.

%Examples of the types of inferences we intend to capture are the following:
%
%\paragraph{Scalar Adjectives.} The {\sc pascal} Recognizing Textual Entailment task (\cite{rte,dagan+glickman+magnini:2005}) requires automatic systems to evaluate the truth or falsity of a 
%statement (the Hypothesis, $H$), given a prior statement (the Text, $T$). The system must decide whether or not $H$ is true or false given $T$, as in:
% \vspace{-.1in}
%
%\eenumsentence{
%\item[$T$:] {\bf Arctic} weather swept across New Jersey. 
% \vspace{-.1in}
%\item [$H$:] The Garden State experienced {\bf cool} temperatures. 
%}
%
% \vspace{-.1in}
%\noindent
%A system which hypothesizes a symmetric synonymy relation between \emph{cool} and \emph{Arctic} would incorrectly infer an entailment relation also if T and H were switched: an awareness of the asymmetry of entailment encoded in our model is crucial to making the correct judgment here.
%In addition, scalar adjectives license inferences based on complex contextual and probabilistic considerations, as in (\ref{4}).
% \vspace{-.1in}
%\eenumsentence{\label{4}
%\item[$T$:] The Empire State Building is {\bf huge}. 
% \vspace{-.1in}
%\item [$H$:] New York City's most famous building is {\bf tall}. 
%}
% \vspace{-.1in}
%Even an RTE system that could manage the difficult coreference task here would generally fail to infer that this is a valid entailment in context, because \emph{huge} does not entail \emph{tall} in a context-independent way. However, these adjectives overlap in the scalar dimensions that they refer to (size, including height as a special case), and a system which is able to recognize this fact as well as the fact that height is the most relevant form of size for a typical building could capture this very common type of inference. 
%
% \vspace{-.15in}
%\paragraph{Adjectives with clausal complements.}  In order to recognize that the Text does not entail the Hypothesis in the following example, it is not enough to recognize events and their participants;  one has additionally to understand the stance the Text takes with respect to the described event:
%
% \vspace{-.1in}
%\eenumsentence{
%\item [$T$:] It is {\bf unlikely} that the attack on the consulate in Benghazi was the work of Al Qaeda.
% \vspace{-.1in}
%\item [$H$:] The attack on the consulate in Benghazi was the work of Al Qaeda.
%}
%
%  \vspace{-.25in}
%\paragraph{Intensional adjectives.} The effect of modifying the nominal head is frequently the introduction of ``epistemic uncertainty'' regarding the description. 
%
% \vspace{-.1in}
%\eenumsentence{
%\item [$T$:] The police arrested the {\bf alleged} criminal.
% \vspace{-.1in}
%\item [$H$:] A criminal was arrested. 
%}
% \vspace{-.1in}
%The inference from $T$ from $H$ would not be justified here, since it is not known whether the allegation is true. However, consider the pair in (\ref{7}):
% \vspace{-.1in}
%
%\eenumsentence{\label{7}
%\item [$T$:] Archeologists discovered an {\bf alleged} paleolithic stone tool. 
% \vspace{-.1in}
%\item [$H$:] A stone tool was discovered. 
%}
%
% \vspace{-.1in}
%\noindent This inference is legitimate because the epistemic scope of the adjective {\it alleged} is the adjective {\it paleolithic}, and not the nominal head  itself (tool). 
%
%%%%
%Supervised or unsupervised annotation is typically used to improve automatic inference based on the lexical items in texts. These annotations reflect the inferential potential associated with lexical items. Some aspects of this potential have been studied in the linguistic literature; computational approaches tend to take these results for granted. 
%In the case of nouns, the WordNet hierarchies have proved useful in numerous studies (e.g., \cite{snow04}); for verbs, lists of special inference patterns have been constructed starting from the work of \cite{kiparsky+kiparsky:1970,karttunen:1971} by \cite{nairn+condoravdi+karttunen:2006,sauri:2008phd,factbank:2009,lotan:2012}. Information about the inferential properties of adjectives is, however, much less easy to come by. 
%For the categories of adjectives that we are interested in, the existing resources have severe shortcomings or are non-existent, in part because the contribution of adjectives tends to be more subtle and more dependent on the rest of the linguistic context. This difficulty requires adopting a more careful methodology for syntactic and semantic lexical categorization tasks. The availability of digital corpora (the biggest one being the Web itself) and crowd-sourcing techniques to elicit the judgments of a larger and more diverse group of native speakers allow us to go beyond the narrow base that traditionally lexical studies were based on. Moreover, the development of statistical modeling techniques allow us to test theoretical hypotheses with large datasets. This should allow us to obtain better data to feed into automatic inferencing systems (such as BiuTee \cite{stern+dagan:2011} or \cite{clark2007role}).
 
%\vspace{-.15in}
%
%\vspace {-2mm}

\section{Theoretical Background}
\vspace{-.2mm}
\subsection{\label{scalar-theory}Scalar adjectives}
\vspace{-.15mm}
As discussed in the introduction, language understanding requires active reconstruction of information which is merely implicit in an utterance, or which can be reconstructed on the basis of an utterance together with its linguistic, social, and worldly context \cite{grice1975logic,hobbs1993interpretation,clark1996using,hanna2003effects,piantadosi2012communicative}.
%
%
%
This is true in particular for scalar adjectives, which have highly context-sensitive meanings (e.g., compare \emph{big baby}, \emph{big tree}, and \emph{big planet}). Many scalar adjectives are organized into entailment scales, where items high on the scale asymmetrically entail items lower on the scale \cite{horn89,horn2000pick}. The use of items lower on the scale, in turn, can lead to a defeasible inference that the sentence would be false if the lower item were replaced by the higher \cite{grice1975logic,horn89}.
 \vspace{-.05in} 
\eenumsentence{
\item warm $<$ hot $<$ scorching
 \vspace{-.05in}
\item \emph{Dallas is scorching} \textbf{entails} \emph{Dallas is hot}, \textbf{which entails} \emph{Dallas is warm}
 \vspace{-.05in}
\item \emph{Dallas is warm} \textbf{may implicate} \emph{Dallas is not hot} and \emph{Dallas is not scorching}
 \vspace{-.05in}
\item \emph{Dallas is hot} \textbf{may implicate} \emph{Dallas is not scorching} 
}

 \vspace{-.05in}
The entailments and pragmatic inferences illustrated here are both important parts of the total understood context of sentences involving scalar adjectives. However, whether the inference arises and how strong it is are highly context-sensitive matters, depending on defeasible assumptions about the speaker's information and the alternative possible utterances that are relevant in context \cite{hirschberg1991theory,frank2012predicting,goodman2013knowledge}. Already in the example above, we see implicatures of varying strength: the speaker's choice to use \emph{warm} will often lead to an inference that the sentence would have been false if \emph{hot} were used, and will likely lead to an even stronger inference that the sentence would have been false if \emph{scorching} were used. 

As a further example of importance of rich linguistic context, compare these three dialogues: 
 \vspace{-.1in}
\begin{multicols}{3}
\eenumsentence{\label{10}
\item Is Dallas scorching?
\vspace{-.1in}
\item It is hot.
}
\eenumsentence{\label{11}
\item Is Dallas hot?
\vspace{-.1in}
\item It is hot.
}
\eenumsentence{\label{12}
\item Is Dallas beautiful?
\vspace{-.45in}
\item It is hot. 
}
\end{multicols}
 \vspace{-.1in}
\noindent
(\ref{10}) illustrates a standard scalar implicature, where the choice to use ``hot'' when ``scorching'' is relevant leads to an inference that Dallas is not scorching. In (\ref{11}), this inference is much less robust, presumably because it is not clear whether ``scorching'' is a relevant alternative. In (\ref{12}) no implicature regarding ``scorching'' arises, but (perhaps surprisingly) there is a robust inference that Dallas is not beautiful. These examples illustrate the importance of taking context into account when drawing inferences from scalar adjectives.

% introduce dimensionality: big/tall
A second complication in modeling inferences from scalar arises due to \textsc{dimensionality}. While adjectives such as \emph{tall} and \emph{heavy} pick out a single scalar dimension (height and weight, respectively), multidimensional adjectives such as \emph{big} and \emph{huge} are more complex, placing requirements on multiple scalar dimensions (e.g., 3-D size and perhaps weight, in the case of \emph{big}). Emotive adjectives such as \emph{happy} are even more complex, since the dimensions that they rely on are not easy to identify or quantify. These relationships among adjectives thus raise difficult questions for an inferential model of adjective semantics. Since it is possible for something to be \emph{big} without being \emph{tall}, 
%(by being large but flat, as in a large city), we know that 
these expressions do not share all of their dimensions and thus are not in an asymmetric entailment relationship as \emph{warm} and \emph{hot} are. But then how do we explain contextual entailments such as the one illustrated in (\ref{4}) above? In addition, there are many cases in which it is simply not clear whether two adjectives are in a scalar relationship: does \emph{happy} asymmetrically entail \emph{content}? Subtle human judgments, gathered under tightly controlled conditions, will be crucial in building and refining a model which will allow NLU systems to approach human performance for such adjectives.

In constructing a formal model of the inferences associated with scalar adjectives, it will be necessary to 
 \vspace{-.25in}
\begin{itemize}
\item [1.] determine which adjective pairs are ``co-scalar'', sharing polarity and all scalar dimensions, and differing only in strength;\footnote{\label{polarity}This is the logical notion of ``polarity'' used in formal semantics \cite{kennedy2001polar}, not the emotive concept from sentiment analysis  \cite{wilson2009recognizing}.}
\item [2.]quantify the difference in strength between co-scalar adjective pairs, in order to predict the strength of implicature (cf.\ \emph{warm/hot/scorching});
\item [3.]determine which adjective pairs overlap partially, differing e.g.\ only in polarity (\emph{cool} vs.\ \emph{warm}) or display partial overlap of dimensions (\emph{big} vs.\ \emph{tall});
\item [4.] Identify factors which influence the weighting of different dimensions in the contextual meaning of adjectives, e.g., the factors which allow people to infer that height is the relevant size dimension in interpreting \emph{big} when buildings are under discussion, but not when cities are.
\end{itemize}
 \vspace{-.05in}

Adjectival polarity for sentiment analysis \cite{williams2009predicting}
correlates only weakly with polarity in our formal sense (see fn.\ \ref{polarity}). More directly related are methods for identifying total entailment relations between adjective pairs developed in \cite{sheinman2009adjscales,sheinmanetal2013}. 
These methods can be improved 
by using semi- and un-supervised methods for learning lexical relations from parsed corpora rather than hand-specified patterns on raw text (cf. \cite{snow04,davidov2008unsupervised,turney2008uniform}). Moreover, there has been virtually no investigation of the other issues that we will consider: the semantic and pragmatic effects of partial scalar overlap, or of the effects of differing scalar distance on the strength of pragmatic inferences. Notably, the use of crowdsourced annotation will be a crucial in enabling us to pursue these fine-grained aspects of lexical structure. This is because quantitative information is needed to discover scalar distance, partial overlap relations, and strength of pragmatic inference in context, and gathering this information requires 
%us to perform 
statistical analyses on the responses of many annotators.

\vspace {-3mm}

\subsection{Veridicity inferences of adjectives with clausal complements}
\vspace {-2mm}

Another common and inferentially rich class of adjectives are predicative adjectives with clausal arguments (\emph{that} {\sc s}, \emph{to} {\sc vp}, or \emph{ing} complements). These adjectives are typically used to communicate an agent's epistemic stance or the likelihood that an eventuality will occur, and some convey in addition an emotive or evaluative attitude of the agent. 
%As we discuss below, 
A precise inferential classification of this class poses interesting challenges.

The relevant agent as well as the type of inferences that arise depend
on both the adjective and its syntactic frame.  The agent with the
implied epistemic stance is sometimes the speaker/writer and sometimes
the referent of the subject of the predicative construction (the
`protagonist').  For instance, \textit{John is sure that Bill left}
ascribes the belief that Bill left to John (the protagonist) and
leaves open what the author thinks, whereas \textit{John is sure to have
left} ascribes a belief to the author. This minimal pair shows that the adjective is not sufficient on its own to determine the type of inference. 
Nor does the syntactic frame determine the type of inference alone: not all adjectives that fit into this frame behave in the same way, as discussed below.

There is no generally accepted syntactic or semantic classification of predicative adjectives taking clausal complements, but three broad classes have been distinguished based on their epistemic inference patterns. 

\vspace{-.2in}
\paragraph{1. Factive adjectives.} These are adjectives  implying that the author is committed to the factuality of the state of affairs described in the complement even when the matrix clause is negated or questioned. They are traditionally analyzed as presupposing the truth of their complement.
Take, for example, (\ref{annoying1}).
%the following sentence. 
 \vspace{-.1in}
\enumsentence{
It is annoying that people post stuff that no one cares about on the web.
\label{annoying1}}
 \vspace{-.1in}
From (\ref{annoying1}), the reader infers that the author presents as true the proposition that people post stuff that nobody cares about on blogs.
This inference is derived directly from the semantics of the adjective {\it annoying}, when used in such a construction. 
Neither negation nor questioning changes the veridicity of the \emph{that }clause, as illustrated in (\ref{factive}).  The focus of the question in (\ref{factive}b) is the evaluation of the \textit{that}-complement as annoying or not.
 \vspace{-.1in}
\enumsentence{
a.  It isn't annoying that people post stuff that no one cares about on blogs. 
\\
b. Is it annoying that people post stuff that no one cares about on blogs?
 \label{factive}
}
 \vspace{-.1in}
\cite{norrick:1978} proposed two subclasses: emotive (e.g.\ \textit{sad}) and evaluative (e.g.\ \textit{stupid}) adjectives. The empirical picture is more complicated, though: the status of the factuality inference varies among speakers (\cite{csli-gang-cssp13} and below).
\vspace{-.2in}
\paragraph{2. Certainty adjectives.} \hspace{-.1in}These adjectives directly encode an agent's degree of certainty in the complement.

\vspace{-0.5em}
\enumsentence{
a. It is certain that people post stuff that no one cares about on blogs.
\\
b.  It is not certain that people post stuff that no one cares about on blogs.\\
 c. Is it certain that people post stuff that no one cares about on blogs? 

\label{certain}
}
 \vspace{-0.5em} 
(\ref{certain}b) has the opposite inference from that of
 (\ref{certain}a), and 
in (\ref{certain}c) it is the
 \textit{that}-complement itself that is questioned. 
Structure-inference patterns for these adjectives
 then would minimally need to distinguish between positive contexts, negative
 contexts and questions.


%Like \textit{certain}, 
Some of the adjectives in this class express absolute certainty or absolute denial of the truth of the embedded clause, and hence give rise to logical entailments; they are \textit{implicative} (\cite{karttunen:1971}). Others, such as \textit{possible, probable, impossible, improbable}, 
%do not express absolute certainty but 
constitute a means for the author  to indicate the probability that (s)he attaches to the factuality of the state of affairs expressed in the embedded clause. In this study, we follow \cite{sauri:2008phd} and approximate this probability by the following scale: {\sc ct}+ (certain), {\sc pr}+ (probable), {\sc ps}+(possible), {\sc u} (none), {\sc pr}- (improbable), {sc pr}- (impossible) and {\sc ct}- (certainly not).

Apart from the adjectives that express an epistemic stance directly, there are adjectives expressing other modalities that have epistemic consequences. These include \textit{able (to), unable (to), willing (to), not willing (to)}, also \textit{unthinkable (that), unbelievable (that)}. Their negative versions may carry negative entailments (\textit{unable to VP} implies that the situation 
described  by the VP complement did not come about), while their positive versions lead to the expectation that the situation described by the complement has occurred or will occur but without warranting an entailment relation.

\vspace{-.2in}
\paragraph{3. Adjectives with no epistemic implications.}
These adjectives fall into several subclasses, e.g. \textit{easy} adjectives, dispositional adjectives such as \textit{afraid (to), keen (to)}, mandative adjectives such as \textit{important (to), essential (to)}, etc. While these do not lead to logical entailments, some of them \textit{invite} the inference that the writer thinks that their complement is factual or at least very likely to have happened. The factors that trigger these invited inferences need further study.

In addition to the extensive study of factive adjectives in \cite{norrick:1978},
there are the more limited studies in \cite{wilkinson:1970} and \cite{barker:2002}. Implicative (\cite{karttunen:1971}) and degree-of-certainty adjectives are only mentioned in passing in the literature. \cite{mindt:2011} looks at the syntax of 51 frequent adjectives taking \emph{that }clauses in the BNC but without any attention to the semantics. \cite{vanlinden+davidse:2009} report on a corpus study of deontic-evaluative adjectives concentrating on \emph{important, essential, crucial}, \emph{appropriate, proper}, and \emph{fitting}.
Pilot studies we have conducted on various subclasses have revealed  that their inferential behavior is dependent on fine-grained structural and contextual factors. We discuss some of our findings to illustrate that getting a proper inferential classification  needs further, systematic study.

\vspace{-.2in}
\paragraph{a. Impersonal constructions of the type [it be ADJ (of NP) to VP].}
 Adjectives in this syntactic pattern can belong to any of three inferential classes described above. 
\cite{norrick:1978} lists several hundred as factive. But even among those 
the situation is more complicated. A sentence like \textit{It was audacious of John to make a trip around the world} readily gets a factive interpretation but one like \textit{It is audacious of anyone to make a trip around the world} very rarely. Our preliminary investigation of the evaluative adjectives among these shows that a factive interpretation reliably arises only in the past tense with a specific \textit{of NP}. For this case we can have the structure-to-inference mapping in (\ref{sti-factive-eval}).

\vspace{-0.5em}
\enumsentence{\label{sti-factive-eval}
$ [ \mbox{ it was } \mbox{ \textit{ADJ}}_{\mbox{\textit{eval}}} \mbox{ of } \mbox{ \textit{NP}}_{\mbox{\textit{spec}}} \mbox{ to } \mbox{ \textit{VP} } ] \; \vDash \; \mbox{ \textit{NP}}_{\mbox{\textit{spec}}} \mbox{ \textit{past} } \mbox{ \textit{VP}} $
}
\vspace{-0.5em}

Adjectives without epistemic entailments may still be interpretated as assigning high probability to the truth of the complement. For instance, \textit{It was essential for researchers to collect accurate information} is judged by MTurk workers to be factual for more than 50\% of them and probable for another 35\%. 
%(other choices were unknown, 50-50, certainly not and probably not). 
Preliminary results thus suggest that for this syntactic pattern there are several subclasses of the three broad, traditionally recognized classes, for which the exact conditioning factors have yet to be identified.

\vspace{-.2in}
\paragraph{b. Personal constructions of the type [NP be ADJ to VP].}
We have discovered that factive adjectives in this frame can be implicative under certain circumstances (\cite{csli-gang-cssp13}). 
The preferred interpretation of \textit{Kim wasn't stupid to send money} is that no money was sent, while that of \textit{Kim wasn't stupid to save money} is the expected factive interpretation. We looked at 60 occurrences of \textit{is/was stupid to} in the enTenTen English corpus, one of the only curated corpora that includes blogs, and found that 25 were clearly factive, 23 clearly implicative and 12 either unclear or part of a different construction. Previous theoretical assumptions would have predicted only the factive interpretations in both cases. We hypothesized that the crucial determinant is whether there is a \emph{harmonic} or a \emph{disharmonic} relationship between the evaluative attitude expressed by the adjective and a general  evaluative assessment of the activity described by the complement clause.  
This  was corroborated by a pilot experimental study. The  pattern is clearly dependent on extra-linguistic factors, since there is no situation-independent metric of stupid or clever actions. 

\vspace{-.2in}
\paragraph{c. Personal and impersonal constructions with a \textit{that}-complements.} 
Our preliminary investigation suggests that \textit{that}-complements of factive adjectives give rise to rather solid factive interpretations but a more detailed study needs to be done.  A preliminary classification of these adjectives is available on-line (\cite{faust-adj-pol-lex}). A structure-to-inference mapping corresponding to an impersonal syntactic frame is given in (\ref{sti-factive}).
\vspace{-0.5em}
\enumsentence{\label{sti-factive}
$ [\mbox{ it be } \mbox{ \textit{ADJ}} \mbox{ that } S \; ] \; \vDash \; S$
}
\vspace{-5mm}
\subsection{Identifying Epistemic Uncertainty: Intensional Adjectives}

\vspace {-1.5mm}

The third adjective class we examine for their inferential properties is the set of non-subsective intensional adjectives. 

This class is further divided into two classes: privatives such as {\it fake} or {\it pretend}, and non-subsective adjectives such as \emph{alleged}. 
Privative adjectives can be analyzed as follows:
\vspace{-0.5em}
 \enumsentence{
 $
 \| A \; N \| \cap \| N \| \; = \; \emptyset $
 }
\vspace{-0.5em}

\noindent
Intensional non-subsective adjectives introduce epistemic uncertainty for the elements within their scope; 
examples include \emph{alleged}, \emph{supposed}, and \emph{presumed}. These adjectives call 
into question some predicative property of the nouns they modify, and no informative inference is associated with this construction \cite{Kamp95prototypetheory}:
\vspace{-.5em}
\enumsentence{
a. $  [ A \; N]  $ (alleged criminal)
\\
b.  $\nvDash  \;  N  $
}
\vspace{-0.5em}

\noindent However, contrary to what is claimed in \cite{amoia2006adjective}, non-subsective adjectives do appear to license specific inferences when examined in a broader context than the [A N] construction usually studied. From preliminary corpus studies of this class\footnote{The initial corpus has been collected from directed CQL queries over  two Sketch Engine corpora, Ententen12 and BNC. Three sentence ``snippets'' have been compiled from this source.}, several distinct patterns of inference emerge. While the typical resulting composition entails  uncertainty of 
 whether the nominal head belongs to the mentioned sortal, (\ref{ex:alleged_criminal}) below, there are many contexts where the epistemic scope is reduced to  a modification or additional attribution of the nominal head, as shown in  (\ref{ex:alleged_paleolithic_tool}). 

\vspace{-0.5em}
\eenumsentence{
	\item The \textbf{alleged criminal} fled the country.
	\label{ex:alleged_criminal}
\vspace{-0.5em}
	\item Archeologists discovered an \textbf{alleged paleolithic tool}.
	\label{ex:alleged_paleolithic_tool}
}
\vspace{-0.5em}

\noindent 
In Example (\ref{ex:alleged_criminal}), the adjective \emph{alleged} calls into question the predicative property of `criminality' of the \emph{criminal}. When a predicative property is called into question by adjectives of this class, are
there any systematic inferences to be made about the semantic field? E.g., is the semantic field still guaranteed to be some hypernym of
\emph{criminal}? Even if the individual does not belong to the set of
``criminals'', it does still seem to belong to the set of ``persons''. In
example (\ref{ex:alleged_paleolithic_tool}), contrastively, at least under one
interpretation, it is whether the \emph{tool} is \emph{paleolithic} or not that
is called into question: i.e., the object belongs to the set of ``tools''
regardless if it is truly \emph{paleolithic} or not.
This inference is schematically represented below.

\vspace{-0.5em}
\enumsentence{
Given the construction $[A_{int} \; N]$, where $A_{int}$ is {\it alleged, ...}, then: \\
a. $[ A_{int}\; N ] \; \nvDash \; N$ \\
b. $[ A_{int} \; A_2\; N ] \; \nvDash \; A_2$\\
c.   $[ A_{int} \; A_2\; N ] \vDash \; N$
\label{AAN}
}
\vspace{-0.5em}

\noindent 
This inference pattern is subject to contextual variables, many of which are not available to sentential compositional mechanisms, but some constraints can be identified. For example, the closer the head noun is to a sortal base level category, such as  {\it bird}, {\it table}, or {\it tool}, the more likely the inference in (\ref{AAN}) will go through:

\vspace{-0.5em}

\enumsentence{
a. The store bought an alleged antique vase. \\
b. The researcher found an alleged Mozart sonata. 
\label{AAN2}}

\vspace{-0.5em}

\noindent These cases make it clear that the epistemic uncertainty in (\ref{AAN2}) involves an additional aspect of the NP,  beyond the unassailable characteristics of the entailed head. That is, the object is clearly a vase (in (\ref{AAN2}a)) and demonstrably a sonata (in (\ref{AAN}b)). Such evidence, however, will not always be available within the composition of a sentence, but will be derivable from context (if at all).  We will refer to the canonical inference in (\ref{AAN}a) as the ``Wide-scope reading'', and the inferences in 
(\ref{AAN}b-c) as the ``Narrow-scope reading''. 

Another interesting distinction emerging in the basic [A N] construction with intensional adjectives is  one based on the   type of the nominal  head. 
The most common semantic types occuring in the corpus are shown below, along with apparent scoping behavior. 

\vspace{-0.5em}
\enumsentence{
a. {\sc event nominal}: 
{\it violation}, {\it misconduct}, {\it murder}, {\it assault}. The more specific nominal descriptions carry greater inferential  force for the hypernym. That is, {\it murder} suggests inference of a death. 
\\
b. {\sc agentive noun}: {\it collaborator}, {\it
perpetrator}, {\it murderer}, {\it criminal}. Epistemic scope is over the
entire sortal.  The canonical form, ``the alleged criminal''. 
\\
c. {\sc undergoer noun}: {\it victim}.  While not
always the case, the scope is narrowed to a modification of the event: For example,  ``the alleged victims of  Whitey Bulger''. 


\label{headtypes}}
\vspace{-0.5em}

Consider the sentences in (\ref{types1}), where {\it alleged} is modifying an event nominal. 

\vspace{-0.5em}
\enumsentence{
a. He denies the alleged assault on the police. 
\\
b. The greatest number of	 alleged	 violations occurred in California. \\
c. He's been charged in connection with the  alleged murder of John Smith, whose mutilated body ...

\label{types1}}
\vspace{-0.5em}

\noindent The inferences associated with (\ref{types1}a-b) follow from the template in (\ref{AAN}a). For 
sentence (\ref{types1}c), however, we need to infer that there was, in fact, a killing, although it is uncertain whether it was a murder. This requires the inference rule below, where the hypernym of the event nominal is infererable from the context.

\vspace{-0.5em}
 \enumsentence{
Given the construction $[A_{int} \; N]$, where $N$ is an event nominal, with certain feature, then: \\
a. $[ A_{int}\; N ] \; \nvDash \; N$ \\
b. $\vDash$ $N'$  where $N \subseteq N'$
}
\vspace{-0.5em}

\noindent We refer to this inference rule as the ``Hypernym reading''. 
Similarly, the scope of an intensional adjective modifying an undergoer can be lowered to a modification of the event description, as  in (\ref{victim}b). 

\vspace{-0.5em}
\enumsentence{
a. Testimony will be heard from the alleged victim in court. 
 \\
 b. The families of two alleged victims of James ``Whitey'' Bulger have received compensation. 
\label{victim}}
\vspace{-0.5em}


\noindent Sentence  (\ref{victim}a) behaves according to the canonical template, while  (\ref{victim}b) involves a narrower scope of the epistemic uncertainty. That is, the inference should be made that there are victims, but the cause (or etiology) of this designation is uncertain. This rule is formally related to that presented above in (\ref{AAN}), where the modification (argument specification, in fact) is postnominal. 

\vspace{-0.5em}
\enumsentence{
Given the construction $[A_{int} \; N \; XP_{mod} ]$, where $XP_{mod}$ is a modification or argument, then: \\
a.  $[ A_{int} \; N \; XP_{mod} ] \; \nvDash \; N  \; XP_{mod} $\\
c.   $[ A_{int} \; N \; XP_{mod}  ] \vDash \; N$
\label{ANXP}
}
\vspace{-0.5em}


\noindent Summarizing the semantic behavior for this class, we have identified at least three distinct structure-to-inference mappings associated with intensional (non-subsective) adjectives. These are:

\vspace{-0.5em}

\enumsentence{
Structure-to-Inference Mappings: \\
a. Wide-scope reading: $\;\;\;\;\;\;\;\;\;$
 $[ A_{int}\; N ] \; \nvDash \; N$ \\
b. Narrow-scope reading 1: $\;\;$
$[ A_{int} \; A_2\; N ] \; \nvDash \; A_2$, $\vDash \; N$
\\
c. Narrow-scope reading 2: $\;\;\;$
$[ A_{int} \; N \; XP_{mod}  ] \vDash \; N$
\\
d. Hypernym reading:  $\;\;\;\;\;\;\;\;\;\;$
$[ A_{int}\; N ] \;  \vDash$ $N'$  where $N \subseteq N'$
}


\subsection{Methodology\label{prelim}}
\vspace {-3mm}

Much work in modern computational linguistics relies on the creation of annotated datasets focused on one or more related linguistic phenomena. Such gold standard corpora are essential for training and tuning the statistical models on which natural language processing tasks largely rely.

In the development of a gold standard corpus using rich linguistic annotation, it is typical to establish an initial model for the phenomena being studied. This includes a model, which is a triple, $M = \langle T,R,I \rangle$, consisting of a vocabulary of terms, $T$, the relations between these terms, $R$, and their interpretation,
$I$. This is often a partial characterization of quite extensive theoretical research in an area, encoded as specification elements for subsequent annotation. These annotations provide the features that are then used for training and testing classification or labeling algorithms over the dataset. Depending on a system's performance, various aspects of the model or related specification will be revised, retrained, and then retested. 
For this reason, we can refer to this methodology as the MATTER cycle: {\it Model}-{\it Annotate}-{\it Train}-{\it Test}-{\it Evaluate}-{\it Revise}  \cite{pustejovsky2012natural}, as illustrated in Figure (\ref{fig:matter}). 

\begin{wrapfigure}{r}{.48\textwidth}
\vspace {-9mm}

  \centering
  \scalebox{.5}{\includegraphics{matter.pdf}}
  \vspace {-4mm}
  \caption{The \textsc{matter} Methodology}
  \label{fig:matter}

  \vspace {-2mm}

\end{wrapfigure}

The ``Model Testing'' phase of this cycle involves iterating over model development followed by subsequent testing by annotation. This  (Model-Annotate)$^*$ technique assumes a classic iterative software development cycle, as applied to the creation of a rich specification language to be used for linguistic annotation.  That is, as issues are encountered with the model when instantiated in a specification and applied to data through the annotation process, the model is revised to accommodate these observations. [Need an arrow between annotate and model]

\medskip
In the present work, we propose a significant enrichment to this methodology. Traditionally, annotation is done by two more or less trained annotators and a trained adjudicator. This is sufficient as long as the phenomena to be annotated fall in reasonable clearcut categories; one does not need many judgments to conclude that native speakers of English interpret \emph{table} as a piece of furniture in a sentence such as \emph{Jane put the food on the table.} But once we get to 
%in order to better model contextual and pragmatic factors that are often ignored or down-played in this strategy. 
linguistic phenomena, such as the adjective classes studied here, reliance on a few judgments might not lead to reliable conclusions because contextual factors and pragmatic effects are critical.  
%% Annie's prose revised by Dan 10/9/13
%For practical reasons, 
Moreover, the corpora that can be created using linguistically trained annotators are rather limited and rarely exhibit all the combinations of relevant context factors, resulting in lack of data capturing what a rich, human-like understanding of texts should be. This limits the usefulness of many machine learning methods that are widely used in natural language understanding \cite{manning1999foundations,wasserman2004all,murphy2012machine}: sophisticated statistical models cannot produce rich understanding without a linguistically informed understanding of what is being modeled. Our project seeks to augment standard annotation practices with both new experimental methods based on crowdsourcing and corpus studies in order to address this lacuna for the important, understudied category of adjectives.

Crowdsourced annotations tasks, when carefully constructed and analyzed, have been shown in previous work to have reliability comparable to traditional expert annotations in some domains including certain RTE tasks \cite{snow:08,munroetal2010} [more refs]. Adopting this method as part of our approach will help us achieve three related goals. First, it makes possible systematic testing of the factors that have been claimed in the linguistic literature to be relevant to the inferences licensed by the use of an adjective, as well as patterns isolated on the basis of existing annotations. Second, statistical analysis of the results of larger-scale annotations gathered by experimental means make it possible to reliably identify inferences which are probabilistic, rather than deterministic, in nature; for example, the defeasible inference that someone described as \emph{attractive} is probably not \emph{stunning}. Third, systematic use of untrained annotators will make available data on the interpretations of people with backgrounds that go beyond those typically involved in standard annotation efforts. We conjecture they will bring in contextual factors that linguists have been trained to ignore.  

%It is well known that 
Isolating the factors that contribute to the perception of an inference is 
%extremely 
notoriously difficult. Testing contrastive contexts with a large number of native speakers is one way to discover the precise factors at work.
%check whether the hypothesized factors are indeed the ones that are at work and whether they have been circumscribed sufficiently.
For instance, from FactBank we learn that 'NP be lucky to VP' has the meaning 'it is highly unlikely that NP VP'. In previous work project consultants Karttunen and Zaenen \cite{karttunen:2012b,zaenen+karttunen:2013} observe that this is far too general, and that the ``unlikely'' meaning is mainly found in future-tense sentences, whereas past tense sentences are overwhelmingly factive. Experimental work has confirmed this while also pointing to further factors that influence the interpretation [new ref:  \cite{Peters et al}]

%However, they show that the facts are more subtle even when tense is taken into account: in (\ref{lucky-1}), the (a) example has the highly unlikely meaning, whereas the (b) example does not (note that replacing `at least' with `in any case' makes the highly unlikely reading again more prominent).
% \vspace{-.1in}
%
%\enumsentence{
%a. Your son will be lucky to escape a jail term. \\
%b.  At least your son will be lucky to escape a jail term.
%\label{lucky-1}}
%\vspace{-.1in}

We take it for granted that in the adjective classes under study the inferential potential is in general influenced in a non-deterministic way by linguistic and non-linguistic context.

%The discovery and treatment of such data involving evaluative adjectives in \cite{karttunen:2012b,zaenen+karttunen:2013,csli-gang-cssp13} relied crucially on a mixture of experimental investigation, standard annotation (done in FactBank), corpus work, and linguistic analysis of the type proposed here, and serves as an example of the promise of the methods. 
%
Five key elements play a role in our methodology:
\eenumsentence{
\vspace{-.1in}
\item The interpretation, I, focuses on structure-to-inference mappings (\textsc{sim}s), indicating how a given adjective type associated with its embedding syntactic environments, which we will call a \textsc{pattern}, contributes to or enables inferences. These \textsc{sim}s will be formulated by experts. The initial \textsc{sim}s will by and large be categorical, making in general a three-way distinction between know, know that not, don't know, [ do we have negative and undetermined sims??]
\moveup
\item The \textsc{sim}s, encoded in the initial interpretation, are checked against large corpus data. This step is heuristic: we analyze large corpora (en-Ten-Ten, CoCa, BNC, the web itself) by means of finite-state templates to find instances of the patterns of our \textsc{sim}s. The templates will depend on the available syntactic analysis of the various corpora. The data collected in this way will inform an updated formulation of the \textsc{sim}s, which are then used to construct experimental material. 
\moveup
\item This experimental material is presented for annotation to Amazon Mechanical Turk workers. The material will cover the combination of features isolated by experts. Annotation instructions will be adapted for use by linguistically untrained native speakers, borrowing methods from experimental psychology, where appropriate. Given the complexity of the factors, the test items will often be larger than one sentence. 
\moveup
\item The heart of the project is the iterative refinement and enrichment of \textsc{sim} models importing new contextual [and probabilistic (question: do the conceptual model include probabilities)] factors. This will be done through the loop of hypothesis construction by the experts and experimental evaluation by native annotators.
\moveup
\moveup
\moveup
\item The product of the iterative refinement is a new annotation standard incorporating probabilistic information. In contrast to traditionally annotations which give a discrete value, based on a few judgments, our methods provide probabilistic information based on large scale annotation. If desired, these probabilities can be reduced to categorical judgments but we hypothesize that using the probabilistic data will improve results for inference depended tasks: for each annotated text fragment our new standard gives a probability distribution of the values a particular [random variable in the] annotation can take. [this needs to be rephrased, i don't have the right terminology].  We also provide a model corpus using this standard. 
\moveup
}

%\vspace{-.1in}
%\noindent 
%We proceed by establishing for each adjective class being studied (Scalar, Veridicity-related, Intensional), 
%an initial model, incorporating the appropriate SIM as the interpretation function, revise these SIMs based on  corpus data and test them through crowdsourcing. The final output is a collection of text snippets with probabilistic annotations


\vspace {-7mm}

\section{Project Plan}

%The project consists of three specific aims, as described in section \ref{intro} above: (1) developing an inferential model for adjectival semantics in natural language; (2) connecting this model to data by formulating templates of structure-to-inference mappings using data mining techniques over Web corpora; and (3) revising and enriching the  theoretical model and inference templates by examining the same data ``in the wild'', that is,  crowdsourced judgments using larger textual contexts. 
\vspace {-3mm}

As described above, we develop for each of the three adjective classes, structure-to-inference mappings. Based on corpus research, we adapt and enrich the existing inferential models for all three types of adjectives. We then select an initial set of target adjectives in each class and construct experimental items based on the SIMs. These are submitted to non-expert annotators (e.g. via Mechanical Turk). The results are analyzed, new hypotheses are incorporated in revised \textsc{sim}, leading to a new set of experiment items and ultimately to a new annotation scheme. 

[SOME THOUGHTS BUT THIS NEEDS MUCH MORE WORK: For the evaluation we extract examples from large corpora and construct an RTE type corpus with HT pairs. Our experts balance this corpus based on the \textsc{sim} information for negative and positive instances, making a prediction about how the (majority of the) annotators will judge the data. These are submitted to the non-expert annotators. On the basis of the results we construct a model corpus that annotates each text snippet with an explicit encoding of the factors that in the final \textsc{sim}s are hypothesized to play a role in the judgments as well as a probability distribution of the non-expert judgments.  

The agreement/disagreement between the predicted annotation and the actual annotation gives a measure of the adequacy of the conceptual models (\textsc{sim}s) we have proposed. [This needs to be developed a bit or left out.] But regardless of the outcome of this evaluation, we will have proposed an annotation standard and a model annotated corpus that can be used for further research. ]

%We then (a) select an initial set of target adjectives, (b) extract text snippets from corpora containing the target adjectives, and (c) construct on this basis small corpora in the format of {\sc rte} to be annotated by both linguistically trained and non-expert annotators. 
%The former will be the ``Gold'' judgments and the latter the ``Wild'' judgments. On the basis of these judgments, we revise our models and test them again ``in the wild.'' 
% Dan: commenting this out since it's been said in other words several times already above.

\vspace {-4mm}

\paragraph{Corpus data.}
We will use a variety of corpus resources, including in some cases the Web, for the extraction of patterns identified as inferentially relevant in the initial model and in subsequent corpus investigations. 
The advantages and disadvantages of using web data vs.\ smaller, more carefully controlled corpora are well-known.
In many cases --- especially when dealing with short patterns whose diagnostic usefulness has already been established --- the size and diversity of styles and genres on the web, as and its access to diverse speaker communities, gives us vital information which compensates for the increase in noise and the possibility of multiple counts.
In other cases, it will be necessary to use resources such as BNC, COCA, and Gigaword, or to develop methods to use these resources to complement each other.
This is especially true in portions of the project which attempt to use semi- and unsupervised methods to identify patterns of interest, where \textsc{pos}-tagged and parsed data is needed, which can be more difficult to obtain starting with web data (see section \ref{scalar} below).
Note further that, where web data is appropriate, our iterative methodology will eliminate many potential false hits from web data and other sources alike, since patterns will be identified as non-diagnostic by human annotators will be removed or downweighted subsequently.

%a subsequent filtering process using the freely available Stanford parser (\cite{marneffe+maccartney+manning:2006}).
%We expect the Web to yield sufficient data so as to allow us to detect outliers based on non-native or idiosyncratic intuitions. We represent the patterns as regular expression ({\sc re}s) and apply them as search queries to the corpus. 

%To avoid missing tokens that exhibit variations of the target patterns, we formulate the pre-defined, ``strict'' patterns in a way that allows flexibility and apply these in addition to the strict patterns. ``Flexibly'' formulated {\sc re}s will likely result in a higher number of false positives. A solution for maximizing the number of true positives while keeping noise to a minimum  is to process the results and perform {\sc pos} tagging and parsing. But rather than processing the entire corpus or all search results returned for a given query, we minimize the computational cost as follows. Search results generated by the strictly formulated {\sc re}s will be considered as valid examples of the pattern and will be directly included in the dataset for semantic analysis. These false positives will be removed from the results returned by the flexibly formulated {\sc re}s for the same pattern, and only the remaining results will be passed to the parser.

\vspace{-.15in}
\paragraph{Experimental data.}
Several studies in the last years have shown that crowd-sourced annotation tasks can deliver reliable results when carefully constructed and analyzed (cf.\ \cite{snow:08,munroetal2010} and section \ref{prelim} above). 
%In addition, we will create and publish non-expert annotation tasks as Amazon Mechanical Turk (AMT) {\sc hit}s, relying on 
%collecting results and evaluating the judgments from MTurkers use boto\footnote{\url{https://github.com/boto/boto}}, a Python interface. 
In constructing tasks on AMT, we will present test questions to ascertain that the workers are native speakers of English, and then ask them to make judgments about inferences (both potential entailments and implicatures) with varying amounts of linguistic context.
Best practices for AMT annotation are not yet firmly established, and we expect that achieving our goals will require us to explore a variety of approaches.
To do so, we will draw on previous NLP research cited above [What is this referring to?]  as well as methods developed in recent experimental cognitive science (one of the Stanford co-PI's areas of research, and a field in which the use of AMT for data collection is firmly established).
Our approach to inferring quantitative patterns of inference as well as inferences associated with specific contexts will rely primarily on this method of data collection, while standard annotations will be used as a tool in building AMT tasks and as a sanity check for the results. 

%The crowd sourcing techniques for the scalar adjectives serve mainly to measure the relative value of each lexical-semantic pattern as a discriminator for adjectives that express different 
%degrees of a given attribute. 
 
\vspace{-.1in}
\subsection{\label{scalar}Scalar Adjectives}

\vspace {-3mm}

%We will begin with experimental and corpus study of two groups of adjectives of interest: a corpus of 100 frequent adjectives with scalar properties expressing different values of twelve different attributes, and a corpus of 300 scalar adjectives for which we have pairwise intensity ratings collected on AMT (10 each). 
\cite{sheinmanetal2013,sheinman2009adjscales} used a methodology similar to Hearst's \cite{hearst1992automatic} to demonstrate the usefulness of  hand-selected syntactic patterns in identifying co-scalarity and relative intensity of adjective pairs. 
\cite{sheinmanetal2013} showed that when adjectives $X$ and $Y$ are ``semantically similar'' according to WordNet --- and so likely to be co-scalar --- it is possible to learn which is stronger by examining the frequency of patterns ``$X$, even $Y$'' and ``if not $Y$, at least $X$''. If these and other carefully chosen patterns are frequent, $Y$ is likely to entail $X$ asymmetrically.

In a recent pilot experiment, we used the Google Web1T corpus with a slightly expanded set of patterns to classify the 300 adjectives for which we have pairwise intensity ratings from AMT. The results indicate both the promise and the need for expansion of this method. If we simply threshold at 40 (the lowest count contained in the corpus), precision is high but recall is low. Many pairs judged by AMT workers to be co-scalar and differing in intensity did not appear, but most pairs returned were intuitively co-scalar and had the expected intensity relation: ``\textbf{indecent} but not \textbf{obscene}''; ``\textbf{sad}, almost \textbf{tragic}''; ``\textbf{unfriendly}, even \textbf{hostile}''; ``\textbf{satisfactory}, and sometimes even \textbf{good}''  --- but this method also returned ``\textbf{good} but not \textbf{easy}''. (Of course, it is not clear that the last result is a false hit rather than an indication that \emph{good} and \emph{easy} are in a context-dependent probabilistic inference relationship: cf.\ discussion in section \ref{scalar-theory} above.)

We propose to extend \cite{sheinmanetal2013} by learning the relevant patterns rather than specifying them by hand. 
This approach follows \cite{snow04,snow2006semantic}, who generalize Hearst's \cite{hearst1992automatic} method of hypernym discovery using WordNet together with a novel learning algorithm applied to a large corpus of dependency-parsed sentences \cite{de2006generating}.
 This approach allowed them to make use of information contained in many patterns that Hearst had not considered, as well as eliminating noise inherent in the use of raw counts; this resulted in a considerable improvement on WordNet's baseline in both precision and recall.

%Inspired by these two approaches, 
We will combine and generalize these methods in several ways. First, we will collect a small gold-standard corpus of judgments of co-scalarity, intuitive strength, entailment, and implicatures among adjective pairs using linguistically trained annotators. Second, we will use adapt these methods to design AMT tasks which allow us to collect a larger corpus of judgments for the 500 most common adjectives in English, using gold-standard judgments also as a sanity check. Third, we will extract all $n$-grams from the Web1T and (1900-) Google Books corpora which contain any two relevant adjectives. Fourth, we will create a large corpus of dependency-parsed text and perform a similar analysis, looking at patterns in dependency relations rather than raw text. Finally, we will use statistical methods to identify, on the basis of these two data sets, which patterns are predictive of the human judgments and to what extent. We will evaluate the resulting model on AMT annotations and corpus data for held-out adjective pairs. 

In addition to the new and linguistically important subject matter, our work contains an important methodological innovation: due to the of the probabilistic nature of many inferences involving scalar adjectives, we will analyze not only \textbf{binary} judgments about entailment and implicature but also the \textbf{probability} that an inference is appropriate, as estimated from quantitative patterns in AMT workers' judgments. This approach offers the hope of capturing the graded nature of many inferences involving scalar adjectives.

With these results in hand, we will revisit the original, context-independent judgments used to build the model, exploring in what ways the inclusion of richer context modulates entailment and implicature judgments. We will also explore methods for predicting context-dependent judgments from tagged and parsed corpora, considering at all available linguistic features of the context. This aspect of the project is likely to be challenging, but we expect that the results of the first section will aid us in identifying relevant features and appropriate learning methods. 



%A given lexically filled pattern will be applied with the lexemes in both orders to identify lexemes that are semantically 
%similar enough to be considered synonyms rather than different in strength or variation inter-speaker variation. 
%Given the pairwise orderings, we construct scales as follows. We process each half of a WordNet ``dumbbell'' 
%(a central adjective like \emph{rich} plus its ``similar'' adjectives \emph{wealthy, comfortable, loaded} etc.) separately. 
%For each pair (centroid, similar-adjective), we instantiate each pattern $p$ in patterns that were extracted in
%the preprocessing stage to obtain phrases $s_{1}=p(\mbox{head-word, similar-word})$
%and $s_{2}=p(\mbox{similar-word, centroid})$. We send $s1$ and $s2$ to a search engine as two separate queries and check whether
%$df\footnote{df represents \concept{document frequency}.}(s_{1})>weight\times df(s_{2})$ and whether $df(s_{1})>threshold$. The higher the values are for the $threshold\footnote{\emph{threshold} regulates the number of pages returned by the search engine that is considered sufficient to trust the result.} $ and $weight\footnote{\concept{weight} regulates the gap between $s_1$ over $s_2$ that is required to prefer one over the other.} $ parameters,
%the more reliable are the results. If $p$ is of the type \concept{intense}, then a positive value is added to the similar-word's score, otherwise
%if $p$ is of the type \concept{mild} a negative value is added. When all the patterns are tested, similar-words with positive values are
%classified as intense, while the similar-words with negative values are classified as mild. Words that score 0 are classified
%as \concept{unconfirmed}. For each pair of words in each one of the subsets (mild and intense), the same procedure is repeated, 
%creating further subsets of \concept{mildest} words that have the most negative values within the mild subset, and \concept{most intense}
%words for the words with the highest positive values within the intense subset.  Adjectives of similar intensity are grouped together.
% 
% \vspace{-1.em}
% 
%\paragraph{The Gold standard for scalar adjectives}
%We plan to collect human judgments from linguistically sophisticated native speakers of the relative intensity of a subset of the adjectives. 
%Our specific goals are (a) to determine whether the ordering derived from the Web data are consistent with human judgments, and (b) 
%which lexical-semantic patterns are more reliable in revealing partial orderings among adjective scalemates. 
%We build on informal pilot work.  
%In one task, we asked Princeton University students to construct scales from randomly ordered groups of the emotion verbs investigated by \cite{mathieu2010verbs}. 
%Results showed good agreement among the judges and between the judges and corpus data (70\% and 77\%, respectively), 
%in line with other semantic similarity judgments. \footnote{Interestingly, the students also judged that one  
%adjective as not being a member of the scale to which it had been pre-assigned in WordNet, indicating the need for a revision of that cluster.} 

%A recent small experiment presented twenty Princeton University students with ten adjectives denoting 
%size (\textit{big, large, enormous, gigantic, huge, tremendous, colossal, gargantuan, monumental, humongous}). 
%The adjectives were arranged two sets of five pairs, one set presenting them in the inverse order from the other set. 
%Two groups of students were each given one set and 
%asked to indicate which member of a given pair expressed a greater value of the attribute ``size;''
%the option of equal value was allowed for as well. Across both groups, there was complete agreement on most pairs (e.g., all 
%students agreed that \textit{gigantic} is greater than \textit{large} and that \textit{huge} is smaller than \textit{tremendous}).
%We found that for the groups agreed well with each other and the judgments for all ratings yielded a coherent picture, as follows: 
%There was complete (100\%) agreement on the weakest and the strongest member of each scale. For other adjectives, 
%there some some disagreement with respect to their pairwise ordering (e.g., whether \emph{colossal} was stronger than 
%\emph{gigantic} or vise versa). The lower frequency of these adjectives may in part account for the lower agreement. 
%
%A subsequent task presenting the same adjectives in random order asked the students to place them 
%on a single scale, again allowing for more than one adjective to occupy the same point on the scale. 
%The scales that the raters constructed were consistent with the pairwise judgments, though the students were not able 
%to consult their pairwise ratings when constructing the scales. 
%All judges rated both \textit{big} and \textit{large} as being the least intensive adjective on the scale. There was also 
%strong agreement that \textit{colossal, gargantuan} and \textit{monumental} expressed the most intense values of "size."
%\textit{Gigantic} and \textit{enormous} were placed towards the end of the scale; 
%some disagreement was found towards the center of the scale, where \textit{humongous} and \textit{tremendous} 
%were not clearly discriminated. In sum, the small experiment confirmed previous findings by \cite{sheinman2009adjscales} and \cite{mathieu2010verbs} 
%and showed that (a) human judges could perform the task, 
%given clear, explicit instructions and illustrative examples; (b) there was good agreement among the judges with respect 
%to the pairwise ordering as well as the scalar orderings; (c) the scales clearly reflected the pairwise orderings; (d) the 
%scales showed the highest agreement at either end and less (though still good) agreement towards the center.
%These pilot studies confirm that speakers have scalar values as part of their representation of adjectives, that 
%they can access these representations and that there is significant agreement across speakers. 
%
%As it is not practicable to collect judgments for the 100 target adjectives with the methodology applied in the pilot work, 
%we collect the Gold standard using the {\sc rte} format.
%In the absence of existing Text-Hypothesis pairs that crucially involve scalar adjectives, we construct  
%pairs such as exemplified below and ask for an evaluation of the Hypothesis, covering fifty frequent adjectives from six scales. 
%(The exact nature of the data will depend on the outcome of the Web searches; some patterns may yield many fewer 
%hits than others and might not be considered further.) 
%
%\enumsentence{
%(T): Pat's daughter is gorgeous.
%\\
%(H): Pat's daughter is nice-looking.
%}
%
%\enumsentence{
%(T): Kim's twin girls are nice-looking
%\\
%(H): Kim's twin girls are gorgeous
%}

%\noindent 
%We then compare the judgments with the data mining results and evaluate their agreement with the different lexical-semantic patterns. 
%We expect some patterns to discriminate more clearly than others among adjectives expressing different intensities of the shared underlying attribute. 

%\paragraph{The ``Wild" standard for scalar adjectives}
%We submit the same T-H pairs to the Turkers and compare their judgments to the Gold standard. Given the large sample 
%of speakers, we do not expect significant differences with respect to the orderings of specific adjective pairs. However, 
%the results will indicate, first, whether specific lexical-semantic patterns better capture speakers' representation of the adjectives' 
%meaning differences and second, whether the patterns that are better discriminators are the same for linguistically 
%sophisticated and linguistically non-expert speakers. If such differences are found, they are likely to guide the future encoding of scalar adjectives 
%in WordNet and the application of WordNet's data to automatic reasoning tasks, where the aim is to train systems to emulate human reasoning. 

\vspace{-.1in}
\subsection{Veridicity of Adjectival Complement Clauses}

\vspace {-3mm}

Ignoring patterns with \emph{-ing} complements, we have currently 45 relevant patterns. We start with \textsc{sim}s based on existing linguistic claims about the nature of these patterns and test it by searching the web and other large corpora. We select an initial set of examples of the pattern and submit them for initial judgments to the research team. On the basis of these judgments we formulate a hypothesis about the use of the pattern, e.g. contrary to existing linguistic literature, the 'NP be evaluative ADJ to VP' pattern is used implicatively as well as factively and the judgment is influenced by harmonicity (see section ...). We then construct experimental data to test that conjecture on the 20 adjectives that are most used with each pattern in the en-ten-ten corpus. We submit these experimentally constructed texts for judgment to MT workers, insuring we get at least 30 judgments per text. We analyze the data, using statistical models to evaluate how well the hypothesized factors account for the experimental results. If necessary, we revise our conjectures and do a new round of experiments. In these revisions we will take into account not only structural and contextual factors but also frequency data in order to more finely characterize the adjective and the patterns. These new experiments deliver an annotated corpus and a revised conceptual model. We distill from this an explicit description of the annotation standard developed in this task.  These results should be amenable to an implementation into systems such as Biutee[refs] 


%We then do a web search for naturally occurring data exhibiting the factors we have discovered to be influencing the inference. On the basis of these web texts we construct an RTE-like test (using the snippets as Text and constructing Hypotheses based on our conjectures) for the 20 most used adjectives, developing at least 5 pairs per pattern. We submit these again to a set of naive native MT workers, insuring we get at least 30 judgments per TH pair. We calculate the probability distribution for each item and produce a model corpus containing for each corpus snippet, the features our conceptual model deems relevant and the probability distribution calculated from the naive native speaker judgments. 


%We will select the 100 most frequent adjectives with clausal complements in the en-Ten-Ten corpus. We will extract 1,000 corpus snippets from the Web based on these adjectives with their frame. (A corpus snippet is a text that contains the sentence in which the target adjective with its pattern occurs and the sentence before and the sentence after it. The en-Ten-Ten corpus, unfortunately, doesnt give enough context). We will balance this corpus so that each adjective occurs at least 10 times. Following the MATTER methodology, linguist annotators will annotate these snippets with their epistemic inference pattern and the factors that are judged to be relevant for this interpretation.
%
%On the basis of these snippets we will construct examples that exhibit variation in the proposed features. These stimuli will be submitted to MTurkers. 
%The possible interpretations will be presented according to two different conditions: in one case the subjects will have to chosen between a positive, a negative and a ``dont know'' answer; in the other, they will choose on the 7-point scale, developed in \cite{sauri:2008phd,sauri-pustejovsky2012} and validated in \cite{demarneff+manning+potts:2012}.
%For inference patterns that are not recognized by linguists we will add follow-up tests, to find out whether the MTurker considers the expression as part of his/her language or not.
%
%On the basis of these judgments we will estimate whether the different features based on the linguistic annotations correspond to those used in the real world. In the cases there is no fit, the linguist annotators will propose new features that will be tested as described above. 
%The annotators will annotate 1,000 new, naturally occurring snippets. These 1,000 snippets will again be annotated by the MTurkers using both the 7-point and the 3-point scale. We will use these data to estimate how well the factors we have isolated capture the MTurkers data by building statistical models.
%


%
%
%\subsection{Clause-selecting Adjectives}
%
%We will develop an initial categorization of the 200 adjectives that most often appear in clausal complements on the basis of the inferential signature(s) most suited to them. To do this we will follow the same method as proposed in the previous section; we explore the web with regular expressions, looking for patterns such as ``It {\sc BE ADJ} to". We extract 2,000 corpus snippets from the Web based on these adjectives with their frame. A corpus snippet is a text that contains the sentence in which the target adjective with its pattern occurs and the sentence before and the sentence after it. We will balance this corpus so that each adjective occurs at least 10 times. Linguist annotators will classify these snippets according to the initial categorization. 
%
%
%
%
%We take 1,000 of these snippets to construct the examples that fit linguistic test patterns. For that, the context following the pattern itself will be changed from the naturally occurring one. For instance, it is easier to judge the factivity of an adjective when it is negated and followed by a negation of the embedded clause. If in ``It wasn't silly for John to make a trip around the world but he didn't go,'' the adjective is taken to be factive, the result is an incoherent discourse.  
%
%We will submit these stimuli to MTurkers with set of instructions similar to the following: 
%%\vspace{-1.0em}
%\begin{itemize}
%\item In each task, you will be shown a statement and you will be given possible interpretations of what the author seems to believe assuming that she is truthful. Select the one that you think represents the author's belief based solely on the statement without making use of any information you might have independently. If the statement does not does not make sense, or if you for some other reason cannot decide, choose the ``Cannot decide" option.
%%\vspace{-1.0em}
%%\item In each task, you will be shown a statement and you will be given possible interpretations. Select the one that you think represents the correct situation. If for some reason you cannot decide, choose the "Cannot decide" option.
%\end{itemize}
%%\vspace{-1.0em}
%The possible interpretations will be presented according to two different conditions: in one case the subjects will have to chosen between a positive, a negative and a ``don't know" answer, in the other, they will chose on the 7-point scale, developed in \cite{sauri:2008phd,sauri-pustejovsky2012} and validated in \cite{demarneff+manning+potts:2012}.
%
%For the patterns that are not recognized by linguists but are found on the Web, we will add follow-up questions, asking: ``Could you imagine yourself or a native English speaker saying the Test Sentence? If not, how would you change the Test Sentence?"
%
%On the basis of these judgments we will be able to calculate whether the different adjective classes we have constructed on the basis of our linguistic criteria correspond to classes that occur in the real world.  This step will allow us to ascertain whether there are substantial differences in the judgments about veridicity as conceived by linguists and those made by non-expert native readers.
%
%The same set of 1,000 snippets will be used without changes in the contexts and submitted to the subjects under the similar conditions as sketched above. This second set will help us isolate the factors that influence judgments in real life. 
%
%On the basis of these crowdsourced judgments, we will be able to construct a model of the structural factors that influence readers judgments and revise the initial categorization. We will then develop annotations guidelines for trained annotators. These annotations will capture the factors that play a role in the categorization. The annotators will annotate 1,000 new, naturally occurring snippets. These 1,000 snippets will be annotated by the MTurkers according to the protocol sketched above using the scale (the 7-point or the 3-point one) which gave the most information. We will use these data to estimate how well the factors we have isolated capture the MTurkers data by building statistical models. 
%


\vspace{-.1in}
\subsection{Intensional Adjectives}
\vspace {-3mm}

 
There are approximately 50 intensional (sub-selective) adjectives that we have identified, from which we will select the most frequent 30 for our investigation. Fewer than 10 of these are root adjectives ({\it superficial}, {\it putative}), and most are participial adjectival derivations, such as {\it alleged}, {\it supposed}, and {\it believed}. For each adjective, we have extracted 100 snippets from the corpus, where snippets are three-sentence fragments from the text. This gives us a corpus of 3,000 snippets for intensional adjectives. 

We will develop an initial classification of 1,000 of these adjectives based on the inferential patterns discussed in the previous section; i.e., wide-scope, narrow-scope, and hypernym readings. 
These are the initial structure-to-inference templates which will constitute the small gold standard. This annotation is performed by undergraduate linguistics majors, with three annotations per snippet. 
That is, we construct the examples that fit the identified test patterns, as shown in (\ref{int-pattern1}) and (\ref{int-pattern2}) below. In these examples, the inference in (\ref{int-pattern1}) is legitimate, while that in (\ref{int-pattern2}) is false. 

\vspace {-3mm}

\begin{multicols}{2}
\enumsentence{
Hypernym Reading: \\
(T): A teenage girl has been arrested over the {\bf alleged murder} of a mourner at a  funeral.% in London. 
\\
(H): A mourner died. 

\label{int-pattern1}
}

\vspace {-6mm}

\enumsentence{
Wide-Scope Reading: \\
(T):  She was soon tried and executed in June by South Korea as an {\bf alleged spy}.
\\
(H): She was a spy. 
\label{int-pattern2}
}
\end{multicols}
\vspace {-2mm}

 \noindent We submit these stimuli to AMT workers with the same guidelines as those given to the linguists. 
 We then submit the remaining 2,000 snippets to both linguists and MTurkers, and examine the differences in judgments. For those cases that do not accord with the pre-assigned classification, we try to isolate the factors contributing to the divergence. 
 To this end, we perform a statistical analysis of the contexts of the adjective for both the cases that are in accordance with the classification and the cases that are not. 
 

\vspace {-3mm}

\subsection{Evaluation}

\vspace {-3mm}

Our evaluations adopt the format of RTE tasks as developed in [need ref to pascal as well as the refs below]. This format has been shown to lead to improvements of NLP inference capabilities and will allow for the further use of our results by teams building NLP systems. 

For scalar adjectives, we propose to evaluate the scales constructed in an {\sc rte} task using methods for measuring the contribution of specific WordNet relations developed by \cite{clark2008using,clark2008augmenting,clark2007role}. 
We will similarly quantify the contribution of scalar orderings among adjectives in WordNet to the {\sc rte} task using a new test set involving adjectives that we have analyzed. 
To perform the evaluation, will encode new scales in WordNet following the model described in 
\cite{sheinmanetal2013}, where WordNet's ``dumbbells'' are augmented with arcs connecting some  adjectives on each half of the dumbbells to specific 
points on the scale. This preserves the original WordNet representation for one central 
adjective (e.g., \emph{rich}) and a set of ``semantically similar'' adjectives (\emph{wealthy, comfortable}, etc.) while also indicating their intensity relative to the central adjective and one another.
This representation is amenable to external evaluation with systems like \cite{clark2007role}.

%Concerning the evaluation of predicative adjectives with clausal complements, we base a first, controlled evaluation on the Brandeis TARSQI system.  We submit our final 1,000 snippets to the system and consider a baseline markup where all the events are factual. We then develop a set of rules based on our findings and evaluate how well the resulting TARSQI system identifies the events.  Larger-scale evaluation proceeds by incorporating our rules and lexical patterns to BiuTee and evaluating the resulting improvement.

%Concerning the evaluation of the predicative adjectives with clausal complements,  we will use
%the same 1,000 snippets and submit them to the Brandeis TARSQI system to mark up the events. We consider all the events as being factual. We develop a set of rules based on our classifications and evaluate how well the resulting system identifies the events using the scale used in the last MTurker experiment. 
%MORE

For the evaluation of the veridicity inference, we do a web search for naturally occurring data exhibiting the factors we have discovered  as being relevant for the inference profiles of our patterns. On the basis of these web texts we construct an RTE-like test (using the snippets as Text and constructing Hypotheses based on our conjectures) for the 20 most used adjectives, developing at least 4 pairs per pattern. We associate with these snippets predictions based on our conceptual model of the pattern and the probability distribution that we have discovered in the previous experiments. We calculate how much these predictions differ from the baseline given by the initial \textsc{sims}. We submit these again to a set of naive native MT workers, insuring we get at least 30 judgments per TH pair. We calculate the new probability distribution for each item and calculate how like it is given the model we assume before the experiment. 


The evaluation for intensional adjectives is similar in approach to that above. We will use the 2,000 snippet corpus  to train both a Naive Bayes and a MaxEnt classifier, where we take all mentions of the adjective to be invoking the   wide-scope reading rule. We take this as our baseline and compare the same two classifiers trained on the differentiated structure-to-inference mappings that were discovered, first by the linguists, and then, as they were enriched by the inferences in the wild. 

%%% new material from Marc, 12/17/12

Finally, the structure-to-inference mappings for all three adjective classes are evaluated by applying the mappings to a held out evaluation set of snippets. We compare the mappings as generated after the corpus mining phase to the revised mappings that were created after analysis of the crowdsourcing results. Additional annotated snippets may be generated for this evaluation if needed. 

\vspace {-2mm}
\vspace {-2mm}
\subsection{Coordination Plan}
\vspace {-2mm}

The PIs at Brandeis, Princeton, and Stanford will maintain regular contact via biweekly Skype conferences. One annual meeting is planned, alternating between Princeton, Brandeis, and Stanford, as well as regular meetings at both national and international conference or workshops focusing on topics of shared interest. 

Lauri Karttunen and Dan Lassiter will responsible for running the experiments at Stanford and will act as advisors on MT experiments for the other sites.  Karttunen will also be an advisor for the construction of experimental materials for the intensional adjectives. Annie Zaenen, Cleo Condoravdi and Lauri Karttunen will design the materials for the clausal complement adjectives and Annie Zaenen will be responsible for the analysis of the data. Dan Lassiter will be a consultant for data analysis on all components of the project. 


\vspace {-2mm}
\vspace {-2mm}

\subsection{Milestones and Deliverables}
\vspace {-1mm}

 
{\bf Year One} of the project is dedicated to: 

\vspace{1mm}\noindent
{\small
\begin{tabularx}{470pt}{|c|X|}

\hline

Q1 & Complete collection of target adjectives; Perform corpus mining;  Collect relevant syntactic patterns for clause-selecting, intensional, and scalar adjectives. 
\\
\hline

Q2 & Derive initial semantic classifications and structure-to-inference mappings; MTurk hit design; coordination of annotation specs; preliminary annotation schema.
 \\

\hline

Q3 & Pilot MTurking experiments;  Evaluate corpus data; Linguists annotate first sets of snippets.    \\
 
\hline

Q4 & Update classifications and mappings; Begin MTurking work; First sets of HIT stimuli for MTurkers; Prepare articles for publication.     \\

\hline

\end{tabularx}
}

\miniskip\noindent
{\bf Year Two} is dedicated to: 

\vspace{2mm}\noindent
{\small
\begin{tabularx}{470pt}{|c|X|}

\hline

Q1 	&  Complete gold standard for expert annotators; Run experiments with MTurkers. \\

\hline

Q2	& Analyze/Evaluate results of MTurker data with/against gold standard. \\

\hline

Q3	&  Continue MTurking work; Update classifications and mappings. \\

\hline

Q4	& Identify detailed contextual parameters accounting for judgment divergence; revise structure-to-inference mappings accordingly; Prepare articles for
       publication;  Organize workshop. \\
\hline

\end{tabularx}
}

%\vspace{.5in}

\miniskip\noindent
{\bf Year Three} is focused on:  

\vspace{2mm}\noindent
{\small
\begin{tabularx}{470pt}{|c|X|}

\hline

Q1 & Revise the annotation specs based on analysis in Y2Q4; develop semantic interpretation of effect of contextual parameters. 
\\

\hline

Q2 & Develop enhanced, layered gold standard.  \\

\hline

Q3 &   Design a way to represent different adjective classes  in WordNet (for scalars, model developed in \cite{sheinmanetal2013} can be developed). \\

\hline

Q4 &  Evaluation; Data collection protocols; Prepare articles for publication.  Final report.\\

\hline

\end{tabularx}
}



%% Outreach
\vspace {-4mm}
\section{Outreach and Education Plan}

\vspace {-3mm}

%The construction of the Gold standard as proposed here involves graduate and undergraduate students, who will learn about the semantic, lexical 
%and syntactic concepts driving our work. The PIs are teaching undergraduate and graduate level courses and will include discussions of 
%adjective semantics and inferencing in future lectures. One PI (Fellbaum) serves as an adviser on a number of Undergraduate Independent 
%Work research projects that focus on scalar adjectives. The other PI (Pustejovsky) serves on the ISO TC 37 /SC 4 committee, and will involve annotator students in  the intial specification  and development of a markup language for adjectivally induced modality. 

 
%% new material
In the early stages of the project we will disseminate information about the paired annotation methodology and the gold standard being developed,   by means of presentations at conferences, workshops, and other meeting venues. We will  exploit the relations we have built up through work in ISO groups for language resources to reach 
%not only 
those in our field and
%but also those 
in related fields such as ontology, linked data, and terminology.  


{\it Adjective Inference Challenge}. To actively engage the community in the adoption and  use of the paired annotation methodology and the resources developed therewith, we will organize an  NLP shared task in the third year of the project,  focused on three specific tasks involving a relatively straightforward challenge,  identifying inferences in textual data associated with the adjective classes being studied. 
The challenge will be run in a way similar to the Shared Tasks of the Conference on Natural Language Learning (CONLL), where colleagues are invited to compete to obtain best results on a specified task and data set. Our challenges will require use of the adjective inference datasets developed for training the competing algorithms.  We plan to host a workshop at the Language Resources and Evaluation Conference (LREC) in May, 2016, where we will engage the community in further refining the scope and nature of deep textual inferences.

{\it Education}. New graduate courses will be developed within the Computer Science Department at Brandeis and  the Linguistics Department at Stanford, associated. The courses, envisioned as  ``Semantic Annotation and Text-based Inference'', taught by the PIs, will have students engage in the methodology developed from the proposal, over new and diverse textual inference phenomena (e.g., bridging, accommodation, shared beliefs). Starting from initial models with expert annotators, students will learn how to deploy the data over a crowdsourced  annotation environment, and examine how to resolve the potential variance or deviation from the initial model. Princeton would contribute materials and  develop a local version of the course after it has been 
offered  once.
%this will link nicely with a Princeton course offered by David Blei, called ``Interacting with Data.'' 
Syllabi and materials from these courses will be made available to the community through mechanisms such as the ACL wiki.
%Association for Computational Linguistics wiki.
%, so that others may use them in their own courses. 
 
{\it Tutorials and Training}.
We will design a tutorial on how the paired annotation methodology can be applied and deployed to other annotation tasks and CL challenges. This will be submitted for inclusion at the major conferences in the field (ACL, NAACL, EACL, AFNLP-sponsored conferences, ICGL, LREC, COLING), beginning in spring, 2015 and continuing to the end of the project. We will also propose tutorials at summer schools such as NASSLLI, ESSLLI, and LSA. 

\vspace {-5mm}
\section{Broader Impact}

\vspace {-3mm}
 
The proposed work  makes several significant contributions to a broader community of computational linguists, AI researchers, and psychologists. 
Our work lays theoretical groundwork for large-scale annotation of three classes of adjectives in order to support automatic systems in inferencing tasks. 
A second contribution is a more sophisticated theory of the role of lexical information in human inferential behavior, a topic of considerable psychological interest.
Third, the work holds out the promise of developing new methodologies for large-scale annotation and combining experimental and corpus investigation that could benefit the development of more human-like systems for natural language understanding.


\vspace {-5mm}
\section{Results from Prior NSF Support}

\vspace {-3mm}

%\miniskip
\noindent
 {\bf SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse} 
{\it NSF 1147912} (PI: James Pustejovsky) 7/2012-6/2015; \$1,962,526.
The goal of this  project is to  build  a comprehensive network of web services and resources within the NLP community. This involves:
(1) the design, development  and promotion of a {\it service-oriented architecture} for NLP development that defines atomic and composite web services for NLP, along with support for service discovery, testing and reuse; (2)  the construction of a {\it Language Application Grid} (LAPPS Grid) based on Service Grid Software developed at NICT and Kyoto Unversity.; (3)  deployment of an open advancement (OA) framework for
component- and application-based evaluation; and  (4)   community involvement with the LAPPS Grid.  

%\miniskip
\noindent
 {\bf 
RI: Small: Interpreting Linguistic Spatiotemporal Relations in Static and Dynamic Contexts}
{\it NSF 1017765} (PI: James Pustejovsky)  8/01/10-7/31/13; 
\$493,862.00. This grant focuses on developing spatial processing algorithms  to automatically capture locations, paths, and motion constructs in text.  Results of this work include the working draft specification of ISO-Space, the implementation of a place identifier, and the mapping of DITL ouput, a dynamic temporal logic, to ISO-Space representations, for subsequent use by extraction and inferencing algorithms. 

%\miniskip
\noindent
{\bf INTEROP: Sustainable Interoperability for Language Technology} 
{\it NSF 0753069} (PI: Nancy Ide; co-PI: James Pustejovsky) 9/2008-8/2013; \$503,620.
This collaborative effort with the EU-funded FLaReNet project is aimed at establishing standards and principles of interoperability within the corpus construction and natural language technology fields, and implementing state-of-the-art formalisms that support interoperability of language processing components and frameworks.  {\bf Publications: }~\cite{idesuderman09};~\cite{ide-bunt:2010:LAW-IV}.
%;~\cite{cieri-etal}.

%\miniskip
\noindent
{\bf CRI: Towards a Comprehensive Linguistic Annotation of Language} {\it CNS 0551615 CRI} (PI James Pustejovsky), awarded 08/22/2005, \$1,935,867.00. This work explored how to merge annotations from different layers of semantic annotation, working from the assumption that it is the combination of these layers that proves useful for applications. This grant spawned two supplementals: (i) CNS 0832940 CRI, awarded 04/03/2008, \$6,000.00, for annotation support, and (ii) CNS 083670 CRI, awarded 05/15/2008, \$10,000.00, to support organization of the North American Computational Linguistic Olympiad (NACLO). {\bf Publications: }~\cite{verhagen-stubbs-pustejovsky:2007:LAW};~\cite{verhagen-EtAl:2007:SemEval-2007};~\cite{verhagen-pustejovsky:2007:Interoperability}.

%\miniskip
\noindent
{\bf Workshop on Scalar Adjectives}
{\it  NSF   1139844}, (PI Christiane Fellbaum).  The PI organized a community workshop on ``Extracting, Constructing, 
Modeling and Applying Scales for Gradable Adjectives'' at the NSF in Virginia, 09/ 30 - 10/011, 2011. Participants agreed that a number of applications, including  
Word Sense Disambiguation,  reasoning and inferencing would benefit 
from the study of scalar adjectives and the encoding of scales in WordNet. The
unidirectional entailments that can be derived from scales and that allow implicatures are
likely to boost deep language understanding. Specific recommendation from workshop participants 
are incorporated into the present proposal. {\bf Publication:}~\cite{sheinmanetal2013}.

%\miniskip
\noindent
{\bf CI-ADDO-EN: A Second-Generation Architecture for WordNet}{\it CNS 0855157}(PI: Christiane Fellbaum) 07/29/2009 - 07/31/2012
\$396,231.00. This grant supports the design and creation of a relational database for WordNet as well as numerous lexicographic improvements 
and community support. {\bf Publications:} \cite{fellbaumvossen2012},\cite{fellbaumencyclopedia},\cite{fellbaumontology2010},\cite{chiarcosinpress},\cite{nikolova2012}.

%\miniskip
\noindent
{\bf CNS: 1204573 CI-P: Collaborative Research: LexLink: Aligning WordNet, FrameNet, PropBank and VerbNet} PI Christiane Fellbaum, awarded 06/01/2002, 
\S45,000.00. This grant funded a community workshop at LREC 2012 to explore the linking of four lexical resources, WordNet, FrameNet, PropBank, VerbNet. 
Participants agreed that the transitive closures among the current partial links would result in numerous benefits for the NLP community. 

%\miniskip
\noindent
{\bf CCF 0937139: Interactive Discovery and Semantic Labeling of Patterns in Spatial Data} PI: T. Funkhauser, co-PIs: D. Blei, A. Finkelstein, C. Fellbaum, awarded 08/25/2009.
\$499,934.00. This work explored the use of WordNet for labeling spatial data. 

%\miniskip
\noindent
Three supplements supported grant IIS -0705199, 08/17/2007 - 07/16/2011: RI: Collaborative Proposal: Complementary Lexical Resources: 
Towards an Alignment of WordNet and FrameNet, PIs C.Fellbaum and C. Baker (ICSI). 
{\bf CNS 0835139}, awarded 06/12/2008, \$6,000.00; {\bf RI: 1007133}, awarded 12/29/2009, \$6,000.00; {\bf IIS 0903358}, awarded 10/31/2008 \S6,000.00. 
The original grant and the three supplements supported the manual alignment of FrameNet and WordNet. An important by-product was the manual 
annotation of all senses of the targeted word forms in the American National Corpus. {\bf Publications:}~\cite{fellbaumbakerLRE};~\cite{bakerfellbaum2008}~\cite{bakerfellbaum2009}~\cite{demelo2012}.


%\miniskip
\noindent
{\bf Workshop on Semantics for Textual Inference}
{\it  NSF  1064068}, (PI Cleo Condoravdi, Co-PI Annie Zaenen). The PIs organized two workshops, one at the LSA Institute 09-10/07/2011, the other at CSLI, Stanford, 09-10/03/2012.
{\bf Publications:}~\cite{Lilt-special-issue}.

 \nocite{zaenen+karttunen:2013}
\nocite{csli-gang-cssp13}
\nocite{csli-gang-cil13}
\nocite{faust-adj-pol-lex}

%
%\section{Results from Prior NSF Support}
%
%
%\textbf{Christiane Fellbaum} is one of the original developers of the WordNet 
%lexical database \cite{Fellbaum98}, \cite{fellbaum1990english} and the current 
%director of the WordNet project. Her work on the development, maintenance and dissemination 
%of WordNet has been funded  by several NSF grants over the past five years. 
%
%Award CNS 0855157 CI-ADDO-EN: A Second-Generation Architecture for WordNet 7/29/2009 (\$396,321), 
%``A Second Generation Architecture for WordNet'' focused on the transformation 
%of the WordNet database from a text-based to a relational (SQLite) database 
%format. The new format makes it possible to encode additional semantic relations and annnotations 
%(including those proposed here)  for words and synsets rather than creating stand-off files. Users in industrial, academic 
%and educational settings will be able to customize WordNet 
%for their own purposes while relying on the Princeton database format. 
%
%One major thrust  of Fellbaum's work has been to make WordNet compatible with other resources 
%serving Natural Language Processing applications. Work on aligning WordNet with the FrameNet database, 
%a complementary lexical database focusing 
%on meaning on the sentence level, was supported by several supplements to an earlier grant 
%(RI: Collaborative Proposal:Complementary Resources: Towards 
%an Alignment of WordNet and FrameNet) supported work for aligning WordNet with 
%FrameNet,  IIS 0835193 6/12/2008  (\$6,000); IIS 0903358 10/31/2008  (\$6,000) and IIS 1007133 12/29/2009 (\$6000). 
%Publications include \cite {FellbaumBaker2008_WNFNinterop}, \cite{FellbaumBakerlinguistics}.
%The alignment of WordNet senses and FrameNet Lexical Units proceeded in parallel with 
%the annotation of the selected word forms in MASC \cite{bakerfellbaum2009}, \cite{deMelo2012LREC}, 
%\cite{Passonneau2012LREC}. Award CNS 1205473 CI-P: Collaborative Research: LexLink: Aligning WordNet, FrameNet, PropBank and VerbNet 6/1/2012 (\$45,000) funded a community workshop at LREC in Istanbul where the alignment of several major lexical resources and its concomitant benefits for NLP were discussed. 
%
%Award CCF 0937139 Interactive Discovery and Semantic Labeling of patterns in Spatial Data 8/25/2009  (\$400,934.00) 
%to Fellbaum and two Princeton colleagues supported efforts to augment visual data with verbal labels and make use 
%of WordNet graph structure to connect meaningful information. 
%
%Fellbaum's most recent award, most closely related to the present proposal, was for organizing a workshop on 
%``Restructuring Adjectives in WordNet (IIS 1139844 6/20/2011 (\$25,000) hosted by NSF, where the corpus-based 
%analysis of scalar adjectives and their representation in WordNet were discussed. Publications include \cite{sheinmanetal2013}. 
%
%
%
%\miniskip\noindent
% {\bf SI2-SSI: The Language Application Grid: A Framework for Rapid Adaptation and Reuse} 
%{\it NSF 1147912} (PI: James Pustejovsky) 7/2012-6/2015; \$1,962,526.
%The goal of this  project is to  build  a comprehensive network of web services and resources within the NLP community. This involves:
%(1) the design, development  and promotion of a {\it service-oriented architecture} for NLP development that defines atomic and composite web services for NLP, along with support for service discovery, testing and reuse; (2)  the construction of a {\it Language Application Grid} (LAPPS Grid) based on Service Grid Software developed at NICT and Kyoto Unversity.; (3)  deployment of an open advancement (OA) framework for
%component- and application-based evaluation; and  (4)  promotion of adoption, use, and community involvement with the LAPPS Grid.  
%
%\miniskip\noindent
% {\bf 
%RI: Small: Interpreting Linguistic Spatiotemporal Relations in Static and Dynamic Contexts}
%{\it NSF 1017765} (PI: James Pustejovsky)  8/01/10-7/31/13; 
%\$493,862.00. This grant focuses on developing spatial processing algorithms  to automatically capture locations, paths, and motion constructs in text.  Results of this work include the working draft specification of ISO-Space, the implementation of a place identifier, and the mapping of DITL ouput, a dynamic temporal logic, to ISO-Space representations, for subsequent use by extraction and inferencing algorithms. 
%
%\miniskip\noindent
%{\bf INTEROP: Sustainable Interoperability for Language Technology} 
%{\it NSF 0753069} (PI: Nancy Ide; co-PI: James Pustejovsky) 9/2008-8/2013; \$503,620.
%This collaborative effort with the EU-funded FLaReNet project is aimed at establishing standards and principles of interoperability within the corpus construction and natural language technology fields, and implementing state-of-the-art formalisms that support interoperability of language processing components and frameworks.  {\bf Publications: }~\cite{idesuderman09};~\cite{ide-bunt:2010:LAW-IV};~\cite{cieri-etal}.
%
%\miniskip\noindent
%{\bf CRI: Towards a Comprehensive Linguistic Annotation of Language} {\it CNS 0551615 CRI} (PI James Pustejovsky), awarded 08/22/2005, \$1,935,867.00. This work explored how to merge annotations from different layers of semantic annotation, working from the assumption that it is the combination of these layers that proves useful for applications. This grant spawned two supplementals: (i) CNS 0832940 CRI, awarded 04/03/2008, \$6,000.00, for annotation support, and (ii) CNS 083670 CRI, awarded 05/15/2008, \$10,000.00, to support organization of the North American Computational Linguistic Olympiad (NACLO). {\bf Publications: }~\cite{verhagen-stubbs-pustejovsky:2007:LAW};~\cite{verhagen-EtAl:2007:SemEval-2007};~\cite{verhagen-pustejovsky:2007:Interoperability}.
%
%\miniskip\noindent
%{\bf Workshop on Scalar Adjectives}
%{\it  NSF   1139844}, (PI Christiane Fellbaum).  The PI organized a community workshop on ``Extracting, Constructing, 
%Modeling and Applying Scales for Gradable Adjectives'' at the NSF in Virginia, 09/ 30 - 10/011, 2011. Participants agreed that a number of applications, including  
%Word Sense Disambiguation,  reasoning and inferencing would benefit 
%from the study of scalar adjectives and the encoding of scales in WordNet. The
%unidirectional entailments that can be derived from scales and that allow implicatures are
%likely to boost deep language understanding. Specific recommendation from workshop participants 
%are incorporated into the present proposal. {\bf Publication:}~\cite{sheinmanetal2013}.
%
%\miniskip\noindent
%{\bf CI-ADDO-EN: A Second-Generation Architecture for WordNet}{\it CNS 0855157}(PI: Christiane Fellbaum) 07/29/2009 - 07/31/2012
%\$396,231.00. This grant supports the design and creation of a relational database for WordNet as well as numerous lexicographic improvements 
%and community support. {\bf Publications:} \cite{fellbaumvossen2012},\cite{fellbaumencyclopedia},\cite{fellbaumontology2010},\cite{chiarcosinpress},\cite{nikolova2012}.
%
%\miniskip\noindent
%{\bf CNS: 1204573 CI-P: Collaborative Research: LexLink: Aligning WordNet, FrameNet, PropBank and VerbNet} PI Christiane Fellbaum, awarded 06/01/2002, 
%\S45,000.00. This grant funded a community workshop at LREC 2012 to explore the linking of four lexical resources, WordNet, FrameNet, PropBank, VerbNet. 
%Participants agreed that the transitive closures among the current partial links would result in numerous benefits for the NLP community. 
%
%\miniskip\noindent
%{\bf CCF 0937139: Interactive Discovery and Semantic Labeling of Patterns in Spatial Data} PI: T. Funkhauser, co-PIs: D. Blei, A. Finkelstein, C. Fellbaum, awarded 08/25/2009.
%\$499,934.00. This work explored the use of WordNet for labeling spatial data. 
%
%\miniskip\noindent
%Three supplements supported grant IIS -0705199, 08/17/2007 - 07/16/2011: RI: Collaborative Proposal: Complementary Lexical Resources: 
%Towards an Alignment of WordNet and FrameNet, PIs C.Fellbaum and C. Baker (ICSI). 
%{\bf CNS 0835139}, awarded 06/12/2008, \$6,000.00; {\bf RI: 1007133}, awarded 12/29/2009, \$6,000.00; {\bf IIS 0903358}, awarded 10/31/2008 \S6,000.00. 
%The original grant and the three supplements supported the manual alignment of FrameNet and WordNet. An important by-product was the manual 
%annotation of all senses of the targeted word forms in the American National Corpus. {\bf Publications:}~\cite{fellbaumbakerLRE};~\cite{bakerfellbaum2008}~\cite{bakerfellbaum2009}~\cite{demelo2012}.
%
% 

%%% References
\newpage
\pagenumbering{arabic}
\renewcommand{\thepage} {E--\arabic{page}}

\bibliography{propbib}
%\bibliographystyle{alpha}
\bibliographystyle{plain}
















\end{document}